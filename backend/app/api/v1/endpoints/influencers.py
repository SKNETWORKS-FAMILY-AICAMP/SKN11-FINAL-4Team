from fastapi import APIRouter, Depends, Query, BackgroundTasks
from sqlalchemy.orm import Session
from typing import List, Optional
import os
import logging

from app.database import get_db
from app.schemas.influencer import (
    AIInfluencer as AIInfluencerSchema,
    AIInfluencerWithDetails,
    AIInfluencerCreate,
    AIInfluencerUpdate,
    StylePreset as StylePresetSchema,
    StylePresetCreate,
    ModelMBTI as ModelMBTISchema,
)
from app.core.security import get_current_user
from app.services.influencers.crud import (
    get_influencers_list,
    get_influencer_by_id,
    create_influencer,
    update_influencer,
    delete_influencer,
)
from app.services.influencers.style_presets import (
    get_style_presets,
    create_style_preset,
)
from app.services.influencers.mbti import get_mbti_list
from app.services.influencers.instagram import (
    InstagramConnectRequest,
    connect_instagram_account,
    disconnect_instagram_account,
    get_instagram_status,
)
from app.services.background_tasks import (
    generate_influencer_qa_background,
    get_background_task_manager,
    BackgroundTaskManager,
)
from fastapi import Request
from app.services.influencers.qa_generator import QAGenerationTask, QAGenerationStatus
from app.services.finetuning_service import (
    get_finetuning_service,
    InfluencerFineTuningService,
)
from datetime import datetime
from app.models.influencer import StylePreset
from fastapi import HTTPException

router = APIRouter()
logger = logging.getLogger(__name__)


@router.get("", response_model=List[AIInfluencerSchema])
async def get_influencers(
    skip: int = Query(0, ge=0),
    limit: int = Query(100, ge=1, le=100),
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """ì‚¬ìš©ìë³„ AI ì¸í”Œë£¨ì–¸ì„œ ëª©ë¡ ì¡°íšŒ"""
    user_id = current_user.get("sub")

    return get_influencers_list(db, user_id, skip, limit)


@router.get("/{influencer_id}", response_model=AIInfluencerWithDetails)
async def get_influencer(
    influencer_id: str,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """íŠ¹ì • AI ì¸í”Œë£¨ì–¸ì„œ ì¡°íšŒ"""
    user_id = current_user.get("sub")
    return get_influencer_by_id(db, user_id, influencer_id)


@router.post("", response_model=AIInfluencerSchema)
async def createnew_influencer(
    influencer_data: AIInfluencerCreate,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """ìƒˆ AI ì¸í”Œë£¨ì–¸ì„œ ìƒì„±"""
    user_id = current_user.get("sub")
    logger.info(
        f"ğŸš€ API: ì¸í”Œë£¨ì–¸ì„œ ìƒì„± ìš”ì²­ - user_id: {user_id}, name: {influencer_data.influencer_name}"
    )

    # ì¸í”Œë£¨ì–¸ì„œ ìƒì„±
    influencer = create_influencer(db, user_id, influencer_data)

    # í™˜ê²½ë³€ìˆ˜ë¡œ ìë™ QA ìƒì„± ì œì–´
    auto_qa_enabled = os.getenv("AUTO_FINETUNING_ENABLED", "true").lower() == "true"
    logger.info(f"ğŸ”§ ìë™ QA ìƒì„± ì„¤ì •: {auto_qa_enabled}")

    if auto_qa_enabled:
        logger.info(
            f"âš¡ ë°±ê·¸ë¼ìš´ë“œ QA ìƒì„± ì‘ì—… ì‹œì‘ - influencer_id: {influencer.influencer_id}"
        )
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ QA ìƒì„± ì‘ì—… ì‹œì‘
        background_tasks.add_task(
            generate_influencer_qa_background, influencer.influencer_id
        )
    else:
        logger.info("â¸ï¸ ìë™ QA ìƒì„±ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤")

    logger.info(f"âœ… API: ì¸í”Œë£¨ì–¸ì„œ ìƒì„± ì™„ë£Œ - ID: {influencer.influencer_id}")
    return influencer


@router.put("/{influencer_id}", response_model=AIInfluencerSchema)
async def update_existing_influencer(
    influencer_id: str,
    influencer_update: AIInfluencerUpdate,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """AI ì¸í”Œë£¨ì–¸ì„œ ì •ë³´ ìˆ˜ì •"""
    user_id = current_user.get("sub")
    return update_influencer(db, user_id, influencer_id, influencer_update)

    # ì¸í”Œë£¨ì–¸ì„œ ì†Œìœ ê¶Œ í™•ì¸ (ì‚¬ìš©ì ì§ì ‘ ì†Œìœ  ë˜ëŠ” íŒ€ ì†Œìœ )
    user = db.query(User).filter(User.user_id == user_id).first()
    if not user:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED, detail="User not found"
        )

    user_group_ids = [group.group_id for group in user.teams]

    query = db.query(AIInfluencer).filter(AIInfluencer.influencer_id == influencer_id)
    if user_group_ids:
        query = query.filter(
            (AIInfluencer.group_id.in_(user_group_ids))
            | (AIInfluencer.user_id == user_id)
        )
    else:
        query = query.filter(AIInfluencer.user_id == user_id)

    influencer = query.first()

    if influencer is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND, detail="Influencer not found"
        )

    # ì—…ë°ì´íŠ¸í•  í•„ë“œë“¤
    update_data = influencer_update.dict(exclude_unset=True)
    for field, value in update_data.items():
        setattr(influencer, field, value)

    db.commit()
    db.refresh(influencer)
    return influencer


@router.delete("/{influencer_id}")
async def delete_existing_influencer(
    influencer_id: str,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """AI ì¸í”Œë£¨ì–¸ì„œ ì‚­ì œ"""
    user_id = current_user.get("sub")
    return delete_influencer(db, user_id, influencer_id)


# ìŠ¤íƒ€ì¼ í”„ë¦¬ì…‹ ê´€ë ¨ API
@router.get("/style-presets/", response_model=List[StylePresetSchema])
async def get_style_presets_list(
    skip: int = Query(0, ge=0),
    limit: int = Query(100, ge=1, le=100),
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """ìŠ¤íƒ€ì¼ í”„ë¦¬ì…‹ ëª©ë¡ ì¡°íšŒ"""
    return get_style_presets(db, skip, limit)


@router.post("/style-presets/", response_model=StylePresetSchema)
async def create_new_style_preset(
    preset_data: StylePresetCreate,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """ìƒˆ ìŠ¤íƒ€ì¼ í”„ë¦¬ì…‹ ìƒì„±"""
    return create_style_preset(db, preset_data)


@router.put("/style-presets/{style_preset_id}", response_model=StylePresetSchema)
async def update_style_preset(
    style_preset_id: str,
    preset_update: StylePresetCreate,  # ë˜ëŠ” ë³„ë„ì˜ Update ìŠ¤í‚¤ë§ˆ ì‚¬ìš© ê°€ëŠ¥
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """ìŠ¤íƒ€ì¼ í”„ë¦¬ì…‹ ìˆ˜ì •"""
    preset = (
        db.query(StylePreset)
        .filter(StylePreset.style_preset_id == style_preset_id)
        .first()
    )
    if not preset:
        raise HTTPException(status_code=404, detail="StylePreset not found")
    for field, value in preset_update.dict(exclude_unset=True).items():
        setattr(preset, field, value)
    db.commit()
    db.refresh(preset)
    return preset


@router.delete("/style-presets/{style_preset_id}")
async def delete_style_preset(
    style_preset_id: str,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """ìŠ¤íƒ€ì¼ í”„ë¦¬ì…‹ ì‚­ì œ"""
    preset = (
        db.query(StylePreset)
        .filter(StylePreset.style_preset_id == style_preset_id)
        .first()
    )
    if not preset:
        raise HTTPException(status_code=404, detail="StylePreset not found")
    db.delete(preset)
    db.commit()
    return {"message": "StylePreset deleted"}


@router.get("/style-presets/{style_preset_id}", response_model=StylePresetSchema)
async def get_style_preset_by_id(
    style_preset_id: str,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """íŠ¹ì • ìŠ¤íƒ€ì¼ í”„ë¦¬ì…‹ ë‹¨ì¼ ì¡°íšŒ"""
    preset = (
        db.query(StylePreset)
        .filter(StylePreset.style_preset_id == style_preset_id)
        .first()
    )
    if not preset:
        raise HTTPException(status_code=404, detail="StylePreset not found")
    return preset


# MBTI ê´€ë ¨ API
@router.get("/mbti/", response_model=List[ModelMBTISchema])
async def get_mbti_options(
    db: Session = Depends(get_db), current_user: dict = Depends(get_current_user)
):
    """MBTI ëª©ë¡ ì¡°íšŒ"""
    return get_mbti_list(db)


# Instagram ë¹„ì¦ˆë‹ˆìŠ¤ ê³„ì • ì—°ë™ ê´€ë ¨ API
@router.post("/{influencer_id}/instagram/connect")
async def connect_instagram_business(
    influencer_id: str,
    request: InstagramConnectRequest,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """AI ì¸í”Œë£¨ì–¸ì„œì— Instagram ë¹„ì¦ˆë‹ˆìŠ¤ ê³„ì • ì—°ë™"""
    user_id = current_user.get("sub")
    print(f"ğŸ” DEBUG request: {request}")
    return await connect_instagram_account(db, user_id, influencer_id, request)


@router.delete("/{influencer_id}/instagram/disconnect")
async def disconnect_instagram_business(
    influencer_id: str,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """AI ì¸í”Œë£¨ì–¸ì„œì—ì„œ Instagram ë¹„ì¦ˆë‹ˆìŠ¤ ê³„ì • ì—°ë™ í•´ì œ"""
    user_id = current_user.get("sub")
    return disconnect_instagram_account(db, user_id, influencer_id)


@router.get("/{influencer_id}/instagram/status")
async def get_instagram_connection_status(
    influencer_id: str,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """AI ì¸í”Œë£¨ì–¸ì„œì˜ Instagram ì—°ë™ ìƒíƒœ ì¡°íšŒ"""
    user_id = current_user.get("sub")
    return await get_instagram_status(db, user_id, influencer_id)


# QA ìƒì„± ê´€ë ¨ API
@router.post("/{influencer_id}/qa/generate")
async def trigger_qa_generation(
    influencer_id: str,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """AI ì¸í”Œë£¨ì–¸ì„œì˜ QA ìƒì„± ìˆ˜ë™ íŠ¸ë¦¬ê±°"""
    user_id = current_user.get("sub")

    # ì¸í”Œë£¨ì–¸ì„œ ì¡´ì¬ í™•ì¸
    influencer = get_influencer_by_id(db, user_id, influencer_id)
    if not influencer:
        from fastapi import HTTPException

        raise HTTPException(status_code=404, detail="ì¸í”Œë£¨ì–¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    # í™˜ê²½ë³€ìˆ˜ë¡œ ìë™ QA ìƒì„± ì œì–´
    auto_qa_enabled = os.getenv("AUTO_FINETUNING_ENABLED", "true").lower() == "true"

    if not auto_qa_enabled:
        from fastapi import HTTPException

        raise HTTPException(
            status_code=403, detail="ìë™ QA ìƒì„±ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤"
        )

    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ QA ìƒì„± ì‘ì—… ì‹œì‘
    background_tasks.add_task(generate_influencer_qa_background, influencer_id)

    return {"message": "QA ìƒì„± ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤", "influencer_id": influencer_id}


@router.get("/{influencer_id}/qa/status")
async def get_qa_generation_status(
    influencer_id: str,
    task_id: Optional[str] = Query(None, description="íŠ¹ì • ì‘ì—… IDë¡œ ì¡°íšŒ"),
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
    task_manager: BackgroundTaskManager = Depends(get_background_task_manager),
):
    """AI ì¸í”Œë£¨ì–¸ì„œì˜ QA ìƒì„± ìƒíƒœ ì¡°íšŒ"""
    user_id = current_user.get("sub")

    # ì¸í”Œë£¨ì–¸ì„œ ì¡´ì¬ í™•ì¸
    influencer = get_influencer_by_id(db, user_id, influencer_id)
    if not influencer:
        from fastapi import HTTPException

        raise HTTPException(status_code=404, detail="ì¸í”Œë£¨ì–¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    from app.models.influencer import BatchKey

    if task_id:
        # íŠ¹ì • ì‘ì—… ìƒíƒœ ì¡°íšŒ (DBì—ì„œ)
        batch_key_entry = db.query(BatchKey).filter(BatchKey.task_id == task_id).first()

        if not batch_key_entry or batch_key_entry.influencer_id != influencer_id:
            from fastapi import HTTPException

            raise HTTPException(status_code=404, detail="ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # ì‹¤ì‹œê°„ OpenAI ë°°ì¹˜ ìƒíƒœ í™•ì¸
        openai_batch_status = None
        if batch_key_entry.openai_batch_id:
            try:
                openai_batch_status = task_manager.qa_generator.check_batch_status(
                    batch_key_entry.openai_batch_id
                )
            except Exception as e:
                openai_batch_status = {"error": f"OpenAI ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"}

        s3_urls = {}
        if batch_key_entry.s3_qa_file_url:
            s3_urls["processed_qa_url"] = batch_key_entry.s3_qa_file_url
        if batch_key_entry.s3_processed_file_url:
            s3_urls["raw_results_url"] = batch_key_entry.s3_processed_file_url

        return {
            "task_id": batch_key_entry.task_id,
            "influencer_id": batch_key_entry.influencer_id,
            "status": batch_key_entry.status,  # DBì—ì„œ ì§ì ‘ ìƒíƒœ ê°€ì ¸ì˜´
            "batch_id": batch_key_entry.openai_batch_id,
            "total_qa_pairs": batch_key_entry.total_qa_pairs,
            "generated_qa_pairs": batch_key_entry.generated_qa_pairs,
            "error_message": batch_key_entry.error_message,
            "s3_urls": s3_urls,
            "created_at": batch_key_entry.created_at,
            "updated_at": batch_key_entry.updated_at,
            "is_running": task_manager.is_task_running(
                task_id
            ),  # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ë§¤ë‹ˆì €ì—ì„œ ì‹¤í–‰ ì—¬ë¶€ í™•ì¸
            "openai_batch_status": openai_batch_status,  # ì‹¤ì œ OpenAI ìƒíƒœ ì¶”ê°€
        }
    else:
        # í•´ë‹¹ ì¸í”Œë£¨ì–¸ì„œì˜ ëª¨ë“  ì‘ì—… ì¡°íšŒ
        all_tasks = task_manager.get_all_qa_tasks()
        influencer_tasks = [
            {
                "task_id": task.task_id,
                "status": task.status.value,
                "batch_id": task.batch_id,
                "total_qa_pairs": task.total_qa_pairs,
                "generated_qa_pairs": task.generated_qa_pairs,
                "error_message": task.error_message,
                "s3_urls": task.s3_urls,
                "created_at": task.created_at,
                "updated_at": task.updated_at,
                "is_running": task_manager.is_task_running(task.task_id),
            }
            for task in all_tasks.values()
            if task.influencer_id == influencer_id
        ]

        return {
            "influencer_id": influencer_id,
            "tasks": influencer_tasks,
            "total_tasks": len(influencer_tasks),
            "running_tasks": len([t for t in influencer_tasks if t["is_running"]]),
        }


@router.delete("/{influencer_id}/qa/tasks/{task_id}")
async def cancel_qa_generation(
    influencer_id: str,
    task_id: str,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
    task_manager: BackgroundTaskManager = Depends(get_background_task_manager),
):
    """AI ì¸í”Œë£¨ì–¸ì„œì˜ QA ìƒì„± ì‘ì—… ì·¨ì†Œ"""
    user_id = current_user.get("sub")

    # ì¸í”Œë£¨ì–¸ì„œ ì¡´ì¬ í™•ì¸
    influencer = get_influencer_by_id(db, user_id, influencer_id)
    if not influencer:
        from fastapi import HTTPException

        raise HTTPException(status_code=404, detail="ì¸í”Œë£¨ì–¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    # ì‘ì—… ì¡´ì¬ í™•ì¸
    task = task_manager.qa_generator.get_task_status(task_id)
    if not task or task.influencer_id != influencer_id:
        from fastapi import HTTPException

        raise HTTPException(status_code=404, detail="ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    # ì‘ì—… ì·¨ì†Œ
    success = task_manager.cancel_task(task_id)

    return {
        "message": (
            "ì‘ì—… ì·¨ì†Œ ìš”ì²­ì´ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤"
            if success
            else "ì‘ì—…ì„ ì·¨ì†Œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        ),
        "task_id": task_id,
        "cancelled": success,
    }


@router.get("/qa/tasks/status")
async def get_all_qa_tasks_status(
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
    task_manager: BackgroundTaskManager = Depends(get_background_task_manager),
):
    """ëª¨ë“  QA ìƒì„± ì‘ì—… ìƒíƒœ ì¡°íšŒ (ê´€ë¦¬ììš©)"""
    all_tasks = task_manager.get_all_qa_tasks()

    return {
        "total_tasks": len(all_tasks),
        "running_tasks": task_manager.get_running_tasks_count(),
        "tasks": [
            {
                "task_id": task.task_id,
                "influencer_id": task.influencer_id,
                "status": task.status.value,
                "batch_id": task.batch_id,
                "total_qa_pairs": task.total_qa_pairs,
                "generated_qa_pairs": task.generated_qa_pairs,
                "error_message": task.error_message,
                "s3_urls": task.s3_urls,
                "created_at": task.created_at,
                "updated_at": task.updated_at,
                "is_running": task_manager.is_task_running(task.task_id),
            }
            for task in all_tasks.values()
        ],
    }


# íŒŒì¸íŠœë‹ ê´€ë ¨ API
@router.get("/{influencer_id}/finetuning/status")
async def get_finetuning_status(
    influencer_id: str,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
    finetuning_service: InfluencerFineTuningService = Depends(get_finetuning_service),
):
    """AI ì¸í”Œë£¨ì–¸ì„œì˜ íŒŒì¸íŠœë‹ ìƒíƒœ ì¡°íšŒ"""
    user_id = current_user.get("sub")

    # ì¸í”Œë£¨ì–¸ì„œ ì¡´ì¬ í™•ì¸
    influencer = get_influencer_by_id(db, user_id, influencer_id)
    if not influencer:
        from fastapi import HTTPException

        raise HTTPException(status_code=404, detail="ì¸í”Œë£¨ì–¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    # í•´ë‹¹ ì¸í”Œë£¨ì–¸ì„œì˜ íŒŒì¸íŠœë‹ ì‘ì—… ì¡°íšŒ
    tasks = finetuning_service.get_tasks_by_influencer(influencer_id)

    return {
        "influencer_id": influencer_id,
        "finetuning_tasks": [
            {
                "task_id": task.task_id,
                "qa_task_id": task.qa_task_id,
                "status": task.status.value,
                "model_name": task.model_name,
                "hf_repo_id": task.hf_repo_id,
                "hf_model_url": task.hf_model_url,
                "error_message": task.error_message,
                "training_epochs": task.training_epochs,
                "created_at": task.created_at,
                "updated_at": task.updated_at,
            }
            for task in tasks
        ],
        "total_tasks": len(tasks),
        "latest_task": tasks[-1].__dict__ if tasks else None,
    }


@router.get("/finetuning/tasks/status")
async def get_all_finetuning_tasks_status(
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
    finetuning_service: InfluencerFineTuningService = Depends(get_finetuning_service),
):
    """ëª¨ë“  íŒŒì¸íŠœë‹ ì‘ì—… ìƒíƒœ ì¡°íšŒ (ê´€ë¦¬ììš©)"""
    all_tasks = finetuning_service.get_all_tasks()

    return {
        "total_tasks": len(all_tasks),
        "tasks": [
            {
                "task_id": task.task_id,
                "influencer_id": task.influencer_id,
                "qa_task_id": task.qa_task_id,
                "status": task.status.value,
                "model_name": task.model_name,
                "hf_repo_id": task.hf_repo_id,
                "hf_model_url": task.hf_model_url,
                "error_message": task.error_message,
                "training_epochs": task.training_epochs,
                "created_at": task.created_at,
                "updated_at": task.updated_at,
            }
            for task in all_tasks.values()
        ],
    }


@router.post("/webhooks/openai/batch-complete")
async def handle_openai_batch_webhook(
    request: Request,
    db: Session = Depends(get_db),
    task_manager: BackgroundTaskManager = Depends(get_background_task_manager),
):
    """OpenAI ë°°ì¹˜ ì‘ì—… ì™„ë£Œ ì›¹í›… ì²˜ë¦¬"""
    try:
        # ì›¹í›… ë°ì´í„° íŒŒì‹±
        webhook_data = await request.json()

        # ë°°ì¹˜ IDì™€ ìƒíƒœ ì¶”ì¶œ
        batch_id = webhook_data.get("data", {}).get("id")
        batch_status = webhook_data.get("data", {}).get("status")

        if not batch_id:
            return {"error": "ë°°ì¹˜ IDê°€ ì—†ìŠµë‹ˆë‹¤"}

        print(f"ğŸ¯ OpenAI ì›¹í›… ìˆ˜ì‹ : batch_id={batch_id}, status={batch_status}")

        # í•´ë‹¹ ë°°ì¹˜ IDë¥¼ ê°€ì§„ ì‘ì—… ì°¾ê¸°
        all_tasks = task_manager.get_all_qa_tasks()
        matching_task = None
        task_id = None

        for tid, task in all_tasks.items():
            if task.batch_id == batch_id:
                matching_task = task
                task_id = tid
                break

        if not matching_task:
            print(f"âš ï¸ í•´ë‹¹ ë°°ì¹˜ IDë¥¼ ê°€ì§„ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: batch_id={batch_id}")
            return {"error": "ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

        print(
            f"âœ… ì‘ì—… ë°œê²¬: task_id={task_id}, influencer_id={matching_task.influencer_id}"
        )

        # ë°°ì¹˜ ì™„ë£Œ ì‹œ ì¦‰ì‹œ ì²˜ë¦¬
        if batch_status == "completed":
            print(f"ğŸš€ ë°°ì¹˜ ì™„ë£Œ, ì¦‰ì‹œ ê²°ê³¼ ì²˜ë¦¬ ì‹œì‘: task_id={task_id}")

            # í™˜ê²½ë³€ìˆ˜ë¡œ ìë™ ì²˜ë¦¬ ì œì–´
            auto_qa_enabled = (
                os.getenv("AUTO_FINETUNING_ENABLED", "true").lower() == "true"
            )

            if not auto_qa_enabled:
                print(
                    f"ğŸ”’ ìë™ QA ì²˜ë¦¬ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤ (AUTO_FINETUNING_ENABLED=false)"
                )
                return {
                    "message": "ìë™ QA ì²˜ë¦¬ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤",
                    "task_id": task_id,
                }

            # ìƒíƒœ ì—…ë°ì´íŠ¸
            matching_task.status = QAGenerationStatus.BATCH_COMPLETED
            matching_task.updated_at = datetime.now()

            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê²°ê³¼ ì²˜ë¦¬ ë° S3 ì—…ë¡œë“œ ì‹¤í–‰
            import asyncio
            from app.database import get_db

            async def process_webhook_result():
                """ì›¹í›… ê²°ê³¼ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë³„ë„ DB ì„¸ì…˜ ì‚¬ìš©"""
                webhook_db = next(get_db())
                try:
                    await task_manager._process_and_upload_results(task_id, webhook_db)
                finally:
                    webhook_db.close()

            asyncio.create_task(process_webhook_result())

            return {"message": "ë°°ì¹˜ ì™„ë£Œ ì›¹í›… ì²˜ë¦¬ ì‹œì‘", "task_id": task_id}

        elif batch_status == "failed":
            print(f"âŒ ë°°ì¹˜ ì‹¤íŒ¨: task_id={task_id}")
            matching_task.status = QAGenerationStatus.FAILED
            matching_task.error_message = "OpenAI ë°°ì¹˜ ì‘ì—… ì‹¤íŒ¨"
            matching_task.updated_at = datetime.now()

            return {"message": "ë°°ì¹˜ ì‹¤íŒ¨ ì²˜ë¦¬ ì™„ë£Œ", "task_id": task_id}

        return {"message": "ì›¹í›… ìˆ˜ì‹ ", "batch_id": batch_id, "status": batch_status}

    except Exception as e:
        print(f"âŒ ì›¹í›… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        import traceback

        print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return {"error": f"ì›¹í›… ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"}
