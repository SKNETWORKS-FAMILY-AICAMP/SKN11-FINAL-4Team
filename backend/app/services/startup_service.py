#!/usr/bin/env python3
"""
μ• ν”λ¦¬μΌ€μ΄μ… μ‹μ‘μ‹ μ‹¤ν–‰λλ” μ„λΉ„μ¤
QA λ°μ΄ν„°κ°€ μμ§€λ§ νμΈνλ‹μ΄ μ‹μ‘λμ§€ μ•μ€ μ‘μ—…λ“¤μ„ μλ™μΌλ΅ μ²λ¦¬
"""

import asyncio
import logging
import os
from typing import List
from sqlalchemy.orm import Session
from datetime import datetime

from app.database import get_db
# batch_job_service μ κ±°λ¨ - BatchKey λ¨λΈ μ§μ ‘ μ‚¬μ©
from app.services.finetuning_service import get_finetuning_service
from app.models.influencer import BatchKey as BatchJob

# λ΅κΉ… μ„¤μ •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class StartupService:
    """μ• ν”λ¦¬μΌ€μ΄μ… μ‹μ‘μ‹ μ‹¤ν–‰λλ” μ„λΉ„μ¤"""
    
    def __init__(self):
# batch_service μ κ±°λ¨
        self.finetuning_service = get_finetuning_service()
    
    async def check_and_restart_finetuning(self) -> int:
        """
        QA λ°μ΄ν„°κ°€ μ—…λ΅λ“λμ—μ§€λ§ νμΈνλ‹μ΄ μ‹μ‘λμ§€ μ•μ€ μ‘μ—…λ“¤μ„ μ°Ύμ•„ μλ™ μ‹μ‘
        Returns:
            μ¬μ‹μ‘λ νμΈνλ‹ μ‘μ—… μ
        """
        # ν™κ²½λ³€μλ΅ μλ™ νμΈνλ‹ λΉ„ν™μ„±ν™” ν™•μΈ
        auto_finetuning_enabled = os.getenv('AUTO_FINETUNING_ENABLED', 'true').lower() == 'true'
        
        if not auto_finetuning_enabled:
            logger.info("π”’ μλ™ νμΈνλ‹μ΄ λΉ„ν™μ„±ν™”λμ–΄ μμµλ‹λ‹¤ (AUTO_FINETUNING_ENABLED=false)")
            return 0
        
        try:
            logger.info("π” μ‹μ‘μ‹ μ„λΉ„μ¤ μ‹¤ν–‰...")
            db: Session = next(get_db())
            
            try:
                # 1. μ§„ν–‰ μ¤‘μ΄λ λ°°μΉ μ‘μ—… μƒνƒ ν™•μΈ λ° μ²λ¦¬
                logger.info("π” μ§„ν–‰ μ¤‘μ΄λ λ°°μΉ μ‘μ—… μƒνƒ ν™•μΈ μ¤‘...")
                in_progress_jobs = db.query(BatchJob).filter(
                    BatchJob.status.in_(['batch_submitted', 'batch_processing']),
                    BatchJob.openai_batch_id.isnot(None)
                ).all()

                if in_progress_jobs:
                    logger.info(f"π― μ²λ¦¬ μ¤‘μ΄μ—λ μ‘μ—… {len(in_progress_jobs)}κ° λ°κ²¬")
                    from app.services.batch_monitor import BatchMonitor # μν™ μ°Έμ΅° λ°©μ§€λ¥Ό μ„ν•΄ μ—¬κΈ°μ„ import
                    monitor = BatchMonitor()
                    for job in in_progress_jobs:
                        await monitor.check_and_update_single_job(job, db)
                    db.commit()
                else:
                    logger.info("β… μ§„ν–‰ μ¤‘μ΄λ λ°°μΉ μ‘μ—… μ—†μ")

                # 2. S3 μ—…λ΅λ“ λ„λ½ μ‘μ—… μ²λ¦¬
                logger.info("π” S3 μ—…λ΅λ“ λ„λ½ μ‘μ—… κ²€μƒ‰ μ¤‘...")
                upload_candidates = db.query(BatchJob).filter(
                    BatchJob.status == "completed",
                    BatchJob.is_uploaded_to_s3 == False,
                    BatchJob.output_file_id.isnot(None)
                ).all()

                if upload_candidates:
                    logger.info(f"π― S3 μ—…λ΅λ“ λ€μƒ {len(upload_candidates)}κ° λ°κ²¬")
                    from app.services.s3_service import get_s3_service
                    from openai import OpenAI
                    import tempfile

                    s3_service = get_s3_service()
                    openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

                    for job in upload_candidates:
                        tmp_file_path = None
                        try:
                            logger.info(f"β¬‡οΈ OpenAI κ²°κ³Ό λ‹¤μ΄λ΅λ“ μ¤‘: output_file_id={job.output_file_id}")
                            file_content = openai_client.files.content(job.output_file_id).read()

                            with tempfile.NamedTemporaryFile(delete=False, mode='wb', suffix=".jsonl") as tmp_file:
                                tmp_file.write(file_content)
                                tmp_file_path = tmp_file.name
                            
                            s3_key = f"influencers/{job.influencer_id}/qa_results/{job.task_id}/generated_qa_results.jsonl"
                            logger.info(f"β¬†οΈ S3 μ—…λ΅λ“ μ¤‘: {s3_key}")
                            s3_url = s3_service.upload_file(tmp_file_path, s3_key)

                            if s3_url:
                                job.s3_qa_file_url = s3_url
                                job.is_uploaded_to_s3 = True
                                job.updated_at = datetime.now()
                                logger.info(f"β… S3 μ—…λ΅λ“ μ„±κ³µ: {s3_url}")
                            else:
                                logger.error(f"β S3 μ—…λ΅λ“ μ‹¤ν¨: task_id={job.task_id}")

                        except Exception as e:
                            logger.error(f"β S3 μ—…λ΅λ“ μ²λ¦¬ μ¤‘ μ¤λ¥: task_id={job.task_id}, error={e}")
                        finally:
                            if tmp_file_path and os.path.exists(tmp_file_path):
                                os.remove(tmp_file_path)
                    
                    db.commit()
                else:
                    logger.info("β… S3 μ—…λ΅λ“ λ„λ½ μ‘μ—… μ—†μ")

                # 3. νμΈνλ‹ μ¬μ‹μ‘ λ€μƒ κ²€μƒ‰
                logger.info("π” νμΈνλ‹ μ¬μ‹μ‘ λ€μƒ κ²€μƒ‰...")
                restart_candidates = db.query(BatchJob).filter(
                    BatchJob.status == "completed",
                    BatchJob.is_uploaded_to_s3 == True,
                    BatchJob.is_finetuning_started == False,
                    BatchJob.s3_qa_file_url.isnot(None)
                ).all()
                
                if not restart_candidates:
                    logger.info("β… μ¬μ‹μ‘ν•  νμΈνλ‹ μ‘μ—…μ΄ μ—†μµλ‹λ‹¤")
                    return 0
                
                logger.info(f"π― μ¬μ‹μ‘ λ€μƒ νμΈνλ‹ μ‘μ—… {len(restart_candidates)}κ° λ°κ²¬")
                
                restarted_count = 0
                
                for batch_job in restart_candidates:
                    try:
                        logger.info(f"π” νμΈνλ‹ μƒνƒ ν™•μΈ: task_id={batch_job.task_id}, influencer_id={batch_job.influencer_id}")
                        
                        # μΈν”λ£¨μ–Έμ„ μ •λ³΄ μ΅°ν
                        from app.services.influencers.crud import get_influencer_by_id
                        influencer_data = get_influencer_by_id(db, "system", batch_job.influencer_id)
                        
                        if not influencer_data:
                            logger.warning(f"β οΈ μΈν”λ£¨μ–Έμ„λ¥Ό μ°Ύμ„ μ μ—†μ: {batch_job.influencer_id}")
                            continue
                        
                        # μ΄λ―Έ νμΈνλ‹μ΄ μ™„λ£λμ—λ”μ§€ ν™•μΈ
                        if self.finetuning_service.is_influencer_finetuned(influencer_data, db):
                            logger.info(f"β… μ΄λ―Έ νμΈνλ‹ μ™„λ£λ¨: influencer_id={batch_job.influencer_id}")
                            # νμΈνλ‹ μ‹μ‘ ν”λκ·Έ μ—…λ°μ΄νΈ (μ¤‘λ³µ μ‹μ‘ λ°©μ§€)
                            batch_job.is_finetuning_started = True
                            db.commit()
                            continue
                        
                        logger.info(f"π€ νμΈνλ‹ μλ™ μ¬μ‹μ‘: task_id={batch_job.task_id}, influencer_id={batch_job.influencer_id}")
                        
                        # νμΈνλ‹ μ‹μ‘
                        success = await self.finetuning_service.start_finetuning_for_influencer(
                            influencer_id=batch_job.influencer_id,
                            s3_qa_file_url=batch_job.s3_qa_file_url,
                            db=db
                        )
                        
                        if success:
                            # νμΈνλ‹ μ‹μ‘ ν‘μ‹ (BatchKey λ¨λΈ μ§μ ‘ μ‚¬μ©)
                            batch_job.is_finetuning_started = True
                            batch_job.updated_at = datetime.now()
                            db.commit()
                            
                            restarted_count += 1
                            logger.info(f"β… νμΈνλ‹ μλ™ μ¬μ‹μ‘ μ™„λ£: task_id={batch_job.task_id}")
                        else:
                            logger.warning(f"β οΈ νμΈνλ‹ μλ™ μ¬μ‹μ‘ μ‹¤ν¨: task_id={batch_job.task_id}")
                    
                    except Exception as e:
                        logger.error(f"β νμΈνλ‹ μ¬μ‹μ‘ μ¤‘ μ¤λ¥: task_id={batch_job.task_id}, error={str(e)}")
                        continue
                
                if restarted_count > 0:
                    logger.info(f"π‰ μ΄ {restarted_count}κ°μ νμΈνλ‹ μ‘μ—… μλ™ μ¬μ‹μ‘ μ™„λ£")
                else:
                    logger.warning("β οΈ μ¬μ‹μ‘ λ€μƒμ΄ μμ—μ§€λ§ λ¨λ‘ μ‹¤ν¨ν–μµλ‹λ‹¤")
                
                return restarted_count
            
            finally:
                db.close()
        
        except Exception as e:
            logger.error(f"β νμΈνλ‹ μ¬μ‹μ‘ κ²€μ‚¬ μ¤‘ μ¤λ¥: {str(e)}", exc_info=True)
            return 0
    
    async def cleanup_old_batch_jobs(self) -> int:
        """μ¤λλ λ°°μΉ μ‘μ—… μ •λ¦¬"""
        try:
            logger.info("π§Ή μ¤λλ λ°°μΉ μ‘μ—… μ •λ¦¬ μ¤‘...")
            
            db: Session = next(get_db())
            
            try:
                # 7μΌ μ΄μƒ λ μ‹¤ν¨ μ‘μ—… μ •λ¦¬ (BatchKey λ¨λΈ μ§μ ‘ μ‚¬μ©)
                from datetime import timedelta
                cutoff_date = datetime.now() - timedelta(days=7)
                
                old_failed_jobs = db.query(BatchJob).filter(
                    BatchJob.status == "failed",
                    BatchJob.updated_at < cutoff_date
                ).all()
                
                cleaned_count = len(old_failed_jobs)
                for job in old_failed_jobs:
                    db.delete(job)
                
                db.commit()
                
                if cleaned_count > 0:
                    logger.info(f"π—‘οΈ {cleaned_count}κ°μ μ¤λλ μ‹¤ν¨ μ‘μ—… μ •λ¦¬ μ™„λ£")
                
                return cleaned_count
            
            finally:
                db.close()
        
        except Exception as e:
            logger.error(f"β λ°°μΉ μ‘μ—… μ •λ¦¬ μ¤‘ μ¤λ¥: {str(e)}", exc_info=True)
            return 0
    
    async def run_startup_tasks(self):
        """μ‹μ‘μ‹ μ‹¤ν–‰ν•  λ¨λ“  μ‘μ—…λ“¤"""
        logger.info("π€ μ• ν”λ¦¬μΌ€μ΄μ… μ‹μ‘μ‹ μ‘μ—… μ‹¤ν–‰ μ¤‘...")
        
        try:
            # 1. νμΈνλ‹ μ¬μ‹μ‘ κ²€μ‚¬
            restarted_count = await self.check_and_restart_finetuning()
            
            # 2. μ¤λλ λ°°μΉ μ‘μ—… μ •λ¦¬
            cleaned_count = await self.cleanup_old_batch_jobs()
            
            logger.info(f"β… μ‹μ‘μ‹ μ‘μ—… μ™„λ£ - μ¬μ‹μ‘: {restarted_count}κ°, μ •λ¦¬: {cleaned_count}κ°")
            
        except Exception as e:
            logger.error(f"β μ‹μ‘μ‹ μ‘μ—… μ‹¤ν–‰ μ¤‘ μ¤λ¥: {str(e)}", exc_info=True)


# κΈ€λ΅λ² μ‹μ‘μ‹ μ„λΉ„μ¤ μΈμ¤ν„΄μ¤
startup_service = StartupService()


def get_startup_service() -> StartupService:
    """μ‹μ‘μ‹ μ„λΉ„μ¤ μμ΅΄μ„± μ£Όμ…"""
    return startup_service


async def run_startup_tasks():
    """μ• ν”λ¦¬μΌ€μ΄μ… μ‹μ‘μ‹ μ‹¤ν–‰ν•  μ‘μ—…λ“¤"""
    service = get_startup_service()
    await service.run_startup_tasks()