"""
ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ê´€ë¦¬ ì„œë¹„ìŠ¤
ì§€ì†ì  ëª¨ë‹ˆí„°ë§ê³¼ S3 ì—…ë¡œë“œë¥¼ í¬í•¨í•œ ë¹„ë™ê¸° ì‘ì—… ì²˜ë¦¬
"""

import asyncio
import logging
import os
from typing import Dict, Optional
from datetime import datetime, timedelta
from sqlalchemy.orm import Session

from app.database import get_db
from app.services.influencers.qa_generator import InfluencerQAGenerator, QAGenerationTask, QAGenerationStatus
from app.services.s3_service import get_s3_service
from app.services.finetuning_service import get_finetuning_service
from app.services.notification_service import get_notification_service
from app.services.batch_job_service import get_batch_job_service

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class BackgroundTaskManager:
    def __init__(self):
        self.qa_generator = InfluencerQAGenerator()
        self.s3_service = get_s3_service()
        self.finetuning_service = get_finetuning_service()
        self.notification_service = get_notification_service()
        self.batch_service = get_batch_job_service()
        self.running_tasks: Dict[str, asyncio.Task] = {}
        self.monitoring_tasks: Dict[str, asyncio.Task] = {}
        self.finetuning_tasks: Dict[str, asyncio.Task] = {}
        
    async def start_qa_generation_task(self, influencer_id: str):
        """
        ì¸í”Œë£¨ì–¸ì„œ QA ìƒì„± ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘
        Args:
            influencer_id: ì¸í”Œë£¨ì–¸ì„œ ID
        """
        try:
            logger.info(f"QA ìƒì„± ì‘ì—… ì‹œì‘: influencer_id={influencer_id}")
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ íšë“
            db: Session = next(get_db())
            
            try:
                # QA ìƒì„± ì‘ì—… ì‹œì‘
                task_id = self.qa_generator.start_qa_generation(influencer_id, db)
                
                # ëª¨ë‹ˆí„°ë§ ë°©ì‹ì„ í™˜ê²½ë³€ìˆ˜ë¡œ ì œì–´
                use_webhook = os.getenv('OPENAI_USE_WEBHOOK', 'true').lower() == 'true'
                
                if use_webhook:
                    # ì›¹í›… ë°©ì‹ - ì£¼ê¸°ì  ëª¨ë‹ˆí„°ë§ ë¹„í™œì„±í™”
                    logger.info(f"âœ… QA ìƒì„± ì‘ì—… ì‹œì‘ ì™„ë£Œ (ì›¹í›… ëŒ€ê¸° ëª¨ë“œ): task_id={task_id}")
                    logger.info(f"ğŸ¯ OpenAI ì›¹í›…ìœ¼ë¡œ ì™„ë£Œ ì•Œë¦¼ì„ ë°›ì„ ì˜ˆì •ì…ë‹ˆë‹¤")
                else:
                    # í´ë§ ë°©ì‹ - ì£¼ê¸°ì  ëª¨ë‹ˆí„°ë§ í™œì„±í™”
                    logger.info(f"ğŸš€ ì§€ì†ì  ëª¨ë‹ˆí„°ë§ ì‘ì—… ìƒì„± ì¤‘ (í´ë§ ëª¨ë“œ): task_id={task_id}")
                    monitor_task = asyncio.create_task(
                        self._continuous_monitor_qa_generation_db(task_id)
                    )
                    self.monitoring_tasks[task_id] = monitor_task
                    self.running_tasks[task_id] = monitor_task
                    
                    logger.info(f"âœ… QA ìƒì„± ì‘ì—… ë° ì§€ì†ì  ëª¨ë‹ˆí„°ë§ ì‹œì‘ ì™„ë£Œ: task_id={task_id}")
                    logger.info(f"ğŸ“ í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë‹ˆí„°ë§ ì‘ì—… ìˆ˜: {len(self.monitoring_tasks)}")
                
            finally:
                db.close()
                
        except Exception as e:
            logger.error(f"QA ìƒì„± ì‘ì—… ì‹œì‘ ì‹¤íŒ¨: influencer_id={influencer_id}, error={str(e)}", exc_info=True)
            # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ì—ì„œëŠ” ì˜ˆì™¸ë¥¼ re-raiseí•˜ì§€ ì•Šê³  ë¡œê¹…ë§Œ í•¨
            pass
    
    async def _continuous_monitor_qa_generation(self, task_id: str):
        """
        QA ìƒì„± ì‘ì—… ì§€ì†ì  ëª¨ë‹ˆí„°ë§ (S3 ì—…ë¡œë“œ í¬í•¨)
        Args:
            task_id: ì‘ì—… ID
        """
        db: Session = None
        try:
            logger.info(f"ì§€ì†ì  QA ìƒì„± ëª¨ë‹ˆí„°ë§ ì‹œì‘: task_id={task_id}")
            
            max_wait_time = timedelta(hours=26)  # ìµœëŒ€ 26ì‹œê°„ ëŒ€ê¸° (ì—¬ìœ ì‹œê°„ í¬í•¨)
            start_time = datetime.now()
            check_interval = 420  # 7ë¶„ë§ˆë‹¤ ìƒíƒœ í™•ì¸ (7ë¶„ = 420ì´ˆ)
            
            while datetime.now() - start_time < max_wait_time:
                try:
                    # ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸
                    logger.info(f"ğŸ”„ ì£¼ê¸°ì  ìƒíƒœ í™•ì¸ ì¤‘: task_id={task_id} (7ë¶„ë§ˆë‹¤)")
                    self.qa_generator.update_task_status(task_id)
                    task = self.qa_generator.get_task_status(task_id)
                    
                    if not task:
                        logger.error(f"ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: task_id={task_id}")
                        break
                    
                    logger.info(f"ğŸ“Š ì‘ì—… ìƒíƒœ: task_id={task_id}, status={task.status.value}, batch_id={task.batch_id}")
                    
                    if task.status == QAGenerationStatus.BATCH_COMPLETED:
                        # ë°°ì¹˜ ì™„ë£Œ ì‹œ ê²°ê³¼ ì²˜ë¦¬ ë° S3 ì—…ë¡œë“œ
                        logger.info(f"ë°°ì¹˜ ì™„ë£Œ ê°ì§€, ê²°ê³¼ ì²˜ë¦¬ ë° S3 ì—…ë¡œë“œ ì‹œì‘: task_id={task_id}")
                        
                        # ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ íšë“
                        db = next(get_db())
                        
                        success = await self._process_and_upload_results(task_id, db)
                        
                        if success:
                            logger.info(f"QA ìƒì„± ë° S3 ì—…ë¡œë“œ ì™„ë£Œ: task_id={task_id}")
                        else:
                            logger.error(f"QA ìƒì„± ê²°ê³¼ ì²˜ë¦¬ ë˜ëŠ” S3 ì—…ë¡œë“œ ì‹¤íŒ¨: task_id={task_id}")
                        break
                        
                    elif task.status == QAGenerationStatus.FAILED:
                        logger.error(f"QA ìƒì„± ì‘ì—… ì‹¤íŒ¨: task_id={task_id}, error={task.error_message}")
                        break
                        
                    elif task.status == QAGenerationStatus.COMPLETED:
                        logger.info(f"QA ìƒì„± ì‘ì—… ì´ë¯¸ ì™„ë£Œë¨: task_id={task_id}")
                        break
                    
                    # ì§€ì •ëœ ê°„ê²©ë§Œí¼ ëŒ€ê¸°
                    logger.info(f"â° ë‹¤ìŒ í™•ì¸ê¹Œì§€ 7ë¶„ ëŒ€ê¸°: task_id={task_id}")
                    await asyncio.sleep(check_interval)
                    
                except Exception as e:
                    logger.error(f"ëª¨ë‹ˆí„°ë§ ë£¨í”„ ì¤‘ ì˜¤ë¥˜: task_id={task_id}, error={str(e)}")
                    await asyncio.sleep(check_interval)  # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ê³„ì† ëª¨ë‹ˆí„°ë§
            
            # ì‹œê°„ ì´ˆê³¼ ì²´í¬
            if datetime.now() - start_time >= max_wait_time:
                logger.warning(f"ëª¨ë‹ˆí„°ë§ ì‹œê°„ ì´ˆê³¼: task_id={task_id}")
                task = self.qa_generator.get_task_status(task_id)
                if task and task.status not in [QAGenerationStatus.COMPLETED, QAGenerationStatus.FAILED]:
                    task.status = QAGenerationStatus.FAILED
                    task.error_message = "ëª¨ë‹ˆí„°ë§ ì‹œê°„ ì´ˆê³¼ (26ì‹œê°„)"
                    task.updated_at = datetime.now()
                
        except Exception as e:
            logger.error(f"ì§€ì†ì  ëª¨ë‹ˆí„°ë§ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: task_id={task_id}, error={str(e)}")
            
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì‘ì—… ìƒíƒœë¥¼ ì‹¤íŒ¨ë¡œ ì—…ë°ì´íŠ¸
            task = self.qa_generator.get_task_status(task_id)
            if task:
                task.status = QAGenerationStatus.FAILED
                task.error_message = f"ëª¨ë‹ˆí„°ë§ ì¹˜ëª…ì  ì˜¤ë¥˜: {str(e)}"
                task.updated_at = datetime.now()
            
        finally:
            # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            if db:
                db.close()
            
            # ëª¨ë‹ˆí„°ë§ ì‘ì—… ì •ë¦¬
            if task_id in self.monitoring_tasks:
                del self.monitoring_tasks[task_id]
            if task_id in self.running_tasks:
                del self.running_tasks[task_id]
                
            logger.info(f"ğŸ ì§€ì†ì  ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ: task_id={task_id}")
            logger.info(f"ğŸ“ ë‚¨ì€ ëª¨ë‹ˆí„°ë§ ì‘ì—… ìˆ˜: {len(self.monitoring_tasks)}")

    async def _continuous_monitor_qa_generation_db(self, task_id: str):
        """
        DB ê¸°ë°˜ QA ìƒì„± ì‘ì—… ì§€ì†ì  ëª¨ë‹ˆí„°ë§ (í´ë§ ëª¨ë“œ)
        Args:
            task_id: ì‘ì—… ID
        """
        db: Session = None
        try:
            logger.info(f"ğŸ”„ DB ê¸°ë°˜ ì§€ì†ì  QA ìƒì„± ëª¨ë‹ˆí„°ë§ ì‹œì‘: task_id={task_id}")
            
            max_wait_time = timedelta(hours=26)  # ìµœëŒ€ 26ì‹œê°„ ëŒ€ê¸°
            start_time = datetime.now()
            check_interval = int(os.getenv('OPENAI_POLLING_INTERVAL', '420'))  # ê¸°ë³¸ 7ë¶„ (420ì´ˆ)
            
            logger.info(f"â° í´ë§ ê°„ê²©: {check_interval}ì´ˆ ({check_interval//60}ë¶„)")
            
            while datetime.now() - start_time < max_wait_time:
                try:
                    # DB ì„¸ì…˜ íšë“
                    db = next(get_db())
                    
                    # DBì—ì„œ ë°°ì¹˜ ì‘ì—… ìƒíƒœ ì¡°íšŒ
                    batch_job = self.batch_service.get_batch_job_by_task_id(db, task_id)
                    if not batch_job:
                        logger.error(f"DBì—ì„œ ë°°ì¹˜ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: task_id={task_id}")
                        break
                    
                    logger.info(f"ğŸ”„ ì£¼ê¸°ì  ìƒíƒœ í™•ì¸ ì¤‘ (DB): task_id={task_id}, status={batch_job.status}")
                    
                    # OpenAIì—ì„œ ì‹¤ì œ ë°°ì¹˜ ìƒíƒœ í™•ì¸
                    openai_status = self.qa_generator.check_batch_status(batch_job.openai_batch_id)
                    
                    # DB ìƒíƒœ ì—…ë°ì´íŠ¸
                    updated_batch = self.batch_service.update_batch_status(
                        db=db,
                        batch_id=batch_job.openai_batch_id,
                        status=openai_status['status'],
                        output_file_id=openai_status.get('output_file_id'),
                        error_message=openai_status.get('error_message')
                    )
                    
                    logger.info(f"ğŸ“Š DB ìƒíƒœ ì—…ë°ì´íŠ¸: task_id={task_id}, status={openai_status['status']}")
                    
                    if openai_status['status'] == 'completed':
                        # ë°°ì¹˜ ì™„ë£Œ ì‹œ ê²°ê³¼ ì²˜ë¦¬ ë° S3 ì—…ë¡œë“œ
                        logger.info(f"âœ… ë°°ì¹˜ ì™„ë£Œ ê°ì§€ (DB), ê²°ê³¼ ì²˜ë¦¬ ì‹œì‘: task_id={task_id}")
                        
                        success = await self._process_and_upload_results_db(task_id, db)
                        
                        if success:
                            logger.info(f"ğŸ‰ QA ìƒì„± ë° S3 ì—…ë¡œë“œ ì™„ë£Œ (DB): task_id={task_id}")
                            # ì™„ë£Œëœ ì‘ì—… ì •ë¦¬
                            self.batch_service.delete_completed_batch_job(db, task_id)
                        else:
                            logger.error(f"âŒ QA ìƒì„± ê²°ê³¼ ì²˜ë¦¬ ì‹¤íŒ¨ (DB): task_id={task_id}")
                        break
                        
                    elif openai_status['status'] == 'failed':
                        logger.error(f"âŒ ë°°ì¹˜ ì‘ì—… ì‹¤íŒ¨ (DB): task_id={task_id}")
                        break
                        
                    elif openai_status['status'] in ['cancelled', 'expired']:
                        logger.warning(f"âš ï¸ ë°°ì¹˜ ì‘ì—… ì·¨ì†Œ/ë§Œë£Œ (DB): task_id={task_id}, status={openai_status['status']}")
                        break
                    
                    # DB ì„¸ì…˜ ì •ë¦¬
                    db.close()
                    db = None
                    
                    # ì§€ì •ëœ ê°„ê²©ë§Œí¼ ëŒ€ê¸°
                    logger.info(f"â° ë‹¤ìŒ í™•ì¸ê¹Œì§€ {check_interval//60}ë¶„ ëŒ€ê¸°: task_id={task_id}")
                    await asyncio.sleep(check_interval)
                    
                except Exception as e:
                    logger.error(f"ëª¨ë‹ˆí„°ë§ ë£¨í”„ ì¤‘ ì˜¤ë¥˜ (DB): task_id={task_id}, error={str(e)}")
                    if db:
                        db.close()
                        db = None
                    await asyncio.sleep(check_interval)  # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ê³„ì† ëª¨ë‹ˆí„°ë§
            
            # ì‹œê°„ ì´ˆê³¼ ì²´í¬
            if datetime.now() - start_time >= max_wait_time:
                logger.warning(f"â° ëª¨ë‹ˆí„°ë§ ì‹œê°„ ì´ˆê³¼ (DB): task_id={task_id}")
                if not db:
                    db = next(get_db())
                self.batch_service.update_batch_status(
                    db=db,
                    batch_id=batch_job.openai_batch_id if batch_job else "",
                    status="failed",
                    error_message="ëª¨ë‹ˆí„°ë§ ì‹œê°„ ì´ˆê³¼ (26ì‹œê°„)"
                )
                
        except Exception as e:
            logger.error(f"ì§€ì†ì  ëª¨ë‹ˆí„°ë§ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ (DB): task_id={task_id}, error={str(e)}")
            
        finally:
            # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            if db:
                db.close()
            
            # ëª¨ë‹ˆí„°ë§ ì‘ì—… ì •ë¦¬
            if task_id in self.monitoring_tasks:
                del self.monitoring_tasks[task_id]
            if task_id in self.running_tasks:
                del self.running_tasks[task_id]
                
            logger.info(f"ğŸ DB ê¸°ë°˜ ì§€ì†ì  ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ: task_id={task_id}")
            logger.info(f"ğŸ“ ë‚¨ì€ ëª¨ë‹ˆí„°ë§ ì‘ì—… ìˆ˜: {len(self.monitoring_tasks)}")

    async def _process_and_upload_results(self, task_id: str, db: Session) -> bool:
        """
        QA ìƒì„± ê²°ê³¼ ì²˜ë¦¬ ë° S3 ì—…ë¡œë“œ
        Args:
            task_id: ì‘ì—… ID
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            task = self.qa_generator.get_task_status(task_id)
            if not task:
                logger.error(f"ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: task_id={task_id}")
                return False
                
            task.status = QAGenerationStatus.PROCESSING_RESULTS
            task.updated_at = datetime.now()
            
            # ë°°ì¹˜ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
            result_file_path = self.qa_generator.download_batch_results(task.batch_id, task_id)
            if not result_file_path:
                raise Exception("ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
            
            # QA ìŒ ì²˜ë¦¬
            qa_pairs = self.qa_generator.process_qa_results(result_file_path)
            if not qa_pairs:
                raise Exception("QA ìŒ ì²˜ë¦¬ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
            # S3ì— ì—…ë¡œë“œ
            if self.s3_service.is_available():
                logger.info(f"S3 ì—…ë¡œë“œ ì‹œì‘: task_id={task_id}, QA ìŒ ê°œìˆ˜={len(qa_pairs)}")
                
                upload_results = self.s3_service.upload_qa_results(
                    influencer_id=task.influencer_id,
                    task_id=task_id,
                    qa_pairs=qa_pairs,
                    raw_results_file=result_file_path
                )
                
                if upload_results.get("processed_qa_url"):
                    logger.info(f"S3 ì—…ë¡œë“œ ì„±ê³µ: {upload_results}")
                    
                    # ì‘ì—…ì— S3 URL ì •ë³´ ì¶”ê°€
                    task.s3_urls = upload_results
                else:
                    logger.warning("S3 ì—…ë¡œë“œ ì‹¤íŒ¨, ë¡œì»¬ì—ë§Œ ì €ì¥ë©ë‹ˆë‹¤")
            else:
                logger.warning("S3 ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ë¡œì»¬ì—ë§Œ ì €ì¥ë©ë‹ˆë‹¤")
            
            # ë¡œì»¬ì—ë„ ì €ì¥ (ë°±ì—…ìš©)
            self.qa_generator.save_qa_pairs_to_db(task.influencer_id, qa_pairs, db)
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            try:
                if os.path.exists(result_file_path):
                    os.remove(result_file_path)
                    logger.info(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ: {result_file_path}")
            except Exception as e:
                logger.warning(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
            
            # ì‘ì—… ì™„ë£Œ
            task.status = QAGenerationStatus.COMPLETED
            task.generated_qa_pairs = len(qa_pairs)
            task.updated_at = datetime.now()
            
            logger.info(f"QA ìƒì„± ë° S3 ì—…ë¡œë“œ ì™„ë£Œ: task_id={task_id}, QA ìŒ={len(qa_pairs)}ê°œ")
            
            # QA ìƒì„± ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ íŒŒì¸íŠœë‹ ì‹œì‘
            if task.s3_urls and task.s3_urls.get("processed_qa_url"):
                logger.info(f"QA ìƒì„± ì™„ë£Œ, íŒŒì¸íŠœë‹ ìë™ ì‹œì‘: task_id={task_id}")
                await self._start_finetuning_after_qa(task_id, task.influencer_id, db)
            
            return True
            
        except Exception as e:
            logger.error(f"ê²°ê³¼ ì²˜ë¦¬ ë° S3 ì—…ë¡œë“œ ì˜¤ë¥˜: task_id={task_id}, error={str(e)}")
            
            task = self.qa_generator.get_task_status(task_id)
            if task:
                task.status = QAGenerationStatus.FAILED
                task.error_message = f"ê²°ê³¼ ì²˜ë¦¬/S3 ì—…ë¡œë“œ ì˜¤ë¥˜: {str(e)}"
                task.updated_at = datetime.now()
            
            return False

    async def _start_finetuning_after_qa(self, qa_task_id: str, influencer_id: str, db: Session):
        """
        QA ìƒì„± ì™„ë£Œ í›„ íŒŒì¸íŠœë‹ ìë™ ì‹œì‘
        Args:
            qa_task_id: QA ìƒì„± ì‘ì—… ID
            influencer_id: ì¸í”Œë£¨ì–¸ì„œ ID
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
        """
        try:
            # QA ì‘ì—… ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            qa_task = self.qa_generator.get_task_status(qa_task_id)
            if not qa_task or not qa_task.s3_urls:
                logger.error(f"QA ì‘ì—… ì •ë³´ê°€ ì—†ê±°ë‚˜ S3 URLì´ ì—†ìŠµë‹ˆë‹¤: {qa_task_id}")
                return
            
            # ì¸í”Œë£¨ì–¸ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            from app.services.influencers.crud import get_influencer_by_id
            user_id = "system"  # ì‹œìŠ¤í…œ ì‘ì—…
            influencer_data = get_influencer_by_id(db, user_id, influencer_id)
            
            if not influencer_data:
                logger.error(f"ì¸í”Œë£¨ì–¸ì„œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {influencer_id}")
                return
            
            # ì¸í”Œë£¨ì–¸ì„œ ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            influencer_dict = {
                "influencer_name": influencer_data.influencer_name,
                "personality": getattr(influencer_data.mbti, 'mbti_traits', 'ì¹œê·¼í•˜ê³  í™œë°œí•œ ì„±ê²©') if influencer_data.mbti else 'ì¹œê·¼í•˜ê³  í™œë°œí•œ ì„±ê²©',
                "style_info": getattr(influencer_data.style_preset, 'influencer_speech', '') if influencer_data.style_preset else ''
            }
            
            # íŒŒì¸íŠœë‹ ì‘ì—… ì‹œì‘
            ft_task_id = self.finetuning_service.start_finetuning_task(
                influencer_id=influencer_id,
                qa_task_id=qa_task_id,
                s3_qa_url=qa_task.s3_urls["processed_qa_url"],
                influencer_data=influencer_dict
            )
            
            # íŒŒì¸íŠœë‹ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘
            ft_task = asyncio.create_task(
                self._execute_finetuning_task(ft_task_id, influencer_dict)
            )
            self.finetuning_tasks[ft_task_id] = ft_task
            
            logger.info(f"íŒŒì¸íŠœë‹ ì‘ì—… ì‹œì‘ë¨: ft_task_id={ft_task_id}")
            
        except Exception as e:
            logger.error(f"íŒŒì¸íŠœë‹ ìë™ ì‹œì‘ ì‹¤íŒ¨: qa_task_id={qa_task_id}, error={str(e)}")

    async def _execute_finetuning_task(self, ft_task_id: str, influencer_data: Dict):
        """
        íŒŒì¸íŠœë‹ ì‘ì—… ì‹¤í–‰
        Args:
            ft_task_id: íŒŒì¸íŠœë‹ ì‘ì—… ID
            influencer_data: ì¸í”Œë£¨ì–¸ì„œ ì •ë³´
        """
        try:
            logger.info(f"íŒŒì¸íŠœë‹ ì‘ì—… ì‹¤í–‰ ì‹œì‘: ft_task_id={ft_task_id}")
            
            # íŒŒì¸íŠœë‹ ì‹¤í–‰ (ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰)
            loop = asyncio.get_event_loop()
            success = await loop.run_in_executor(
                None, 
                self.finetuning_service.execute_finetuning_task,
                ft_task_id, 
                influencer_data
            )
            
            if success:
                logger.info(f"íŒŒì¸íŠœë‹ ì‘ì—… ì™„ë£Œ: ft_task_id={ft_task_id}")
                
                # íŒŒì¸íŠœë‹ ì™„ë£Œ í›„ ì•Œë¦¼ ì „ì†¡
                await self._send_finetuning_completion_notification(ft_task_id)
            else:
                logger.error(f"íŒŒì¸íŠœë‹ ì‘ì—… ì‹¤íŒ¨: ft_task_id={ft_task_id}")
                
        except Exception as e:
            logger.error(f"íŒŒì¸íŠœë‹ ì‘ì—… ì‹¤í–‰ ì˜¤ë¥˜: ft_task_id={ft_task_id}, error={str(e)}")
            
            # ì‹¤íŒ¨ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
            ft_task = self.finetuning_service.get_task_status(ft_task_id)
            if ft_task:
                from app.services.finetuning_service import FineTuningStatus
                ft_task.status = FineTuningStatus.FAILED
                ft_task.error_message = str(e)
                ft_task.updated_at = datetime.now()
        
        finally:
            # ì‘ì—… ì •ë¦¬
            if ft_task_id in self.finetuning_tasks:
                del self.finetuning_tasks[ft_task_id]

    async def _send_finetuning_completion_notification(self, ft_task_id: str):
        """
        íŒŒì¸íŠœë‹ ì™„ë£Œ ì•Œë¦¼ ì „ì†¡
        Args:
            ft_task_id: íŒŒì¸íŠœë‹ ì‘ì—… ID
        """
        try:
            ft_task = self.finetuning_service.get_task_status(ft_task_id)
            if not ft_task:
                logger.error(f"íŒŒì¸íŠœë‹ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ft_task_id}")
                return
            
            # ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            db: Session = next(get_db())
            try:
                from app.services.influencers.crud import get_influencer_by_id
                user_id = "system"  # ì‹œìŠ¤í…œ ì‘ì—…ìœ¼ë¡œ ì¡°íšŒ
                influencer_data = get_influencer_by_id(db, user_id, ft_task.influencer_id)
                
                if not influencer_data:
                    logger.error(f"ì¸í”Œë£¨ì–¸ì„œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ft_task.influencer_id}")
                    return
                
                # ì‹¤ì œ ì‚¬ìš©ì IDëŠ” ì¸í”Œë£¨ì–¸ì„œì˜ user_id í•„ë“œì—ì„œ ê°€ì ¸ì˜¤ê¸°
                actual_user_id = influencer_data.user_id
                
                # TODO: ì‹¤ì œ ì‚¬ìš©ì ì´ë©”ì¼ ì£¼ì†Œ ê°€ì ¸ì˜¤ê¸° (User í…Œì´ë¸”ì—ì„œ)
                # í˜„ì¬ëŠ” ë”ë¯¸ ì´ë©”ì¼ ì‚¬ìš©
                user_email = f"user_{actual_user_id}@example.com"  # ì‹¤ì œ êµ¬í˜„ ì‹œ User í…Œì´ë¸”ì—ì„œ ì¡°íšŒ
                
                # ì•Œë¦¼ ì „ì†¡
                await self.notification_service.send_finetuning_completion_notification(
                    user_email=user_email,
                    user_id=actual_user_id,
                    influencer_name=influencer_data.influencer_name,
                    model_url=ft_task.hf_model_url or "https://huggingface.co/model"
                )
                
                logger.info(f"íŒŒì¸íŠœë‹ ì™„ë£Œ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ: {ft_task_id} -> {actual_user_id}")
                
            finally:
                db.close()
            
        except Exception as e:
            logger.error(f"íŒŒì¸íŠœë‹ ì™„ë£Œ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {e}")

    async def _process_and_upload_results_db(self, task_id: str, db: Session) -> bool:
        """
        DB ê¸°ë°˜ QA ìƒì„± ê²°ê³¼ ì²˜ë¦¬ ë° S3 ì—…ë¡œë“œ (í´ë§ ëª¨ë“œ)
        Args:
            task_id: ì‘ì—… ID
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
        Returns:
            ì²˜ë¦¬ ì„±ê³µ ì—¬ë¶€
        """
        try:
            # DBì—ì„œ ë°°ì¹˜ ì‘ì—… ì¡°íšŒ
            batch_job = self.batch_service.get_batch_job_by_task_id(db, task_id)
            if not batch_job:
                logger.error(f"DBì—ì„œ ë°°ì¹˜ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: task_id={task_id}")
                return False
            
            if batch_job.is_processed:
                logger.info(f"ì´ë¯¸ ì²˜ë¦¬ëœ ì‘ì—…: task_id={task_id}")
                return True
            
            logger.info(f"ğŸ”„ QA ê²°ê³¼ ì²˜ë¦¬ ì‹œì‘ (DB): task_id={task_id}")
            
            # ë°°ì¹˜ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
            result_file_path = self.qa_generator.download_batch_results(batch_job.openai_batch_id, task_id)
            if not result_file_path:
                logger.error(f"ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: task_id={task_id}")
                return False
            
            # QA ìŒ ì²˜ë¦¬
            qa_pairs = self.qa_generator.process_qa_results(result_file_path)
            if not qa_pairs:
                logger.error(f"QA ìŒ ì²˜ë¦¬ ì‹¤íŒ¨: task_id={task_id}")
                return False
            
            logger.info(f"ğŸ“Š QA ìŒ ì²˜ë¦¬ ì™„ë£Œ: {len(qa_pairs)}ê°œ, task_id={task_id}")
            
            # S3ì— QA ê²°ê³¼ ì—…ë¡œë“œ
            s3_urls = self.s3_service.upload_qa_results(
                influencer_id=batch_job.influencer_id,
                task_id=task_id,
                qa_pairs=qa_pairs,
                raw_results_file=result_file_path
            )
            
            if s3_urls.get("qa_file_url"):
                logger.info(f"ğŸ“¤ S3 ì—…ë¡œë“œ ì„±ê³µ: task_id={task_id}")
                
                # DBì— ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ
                self.batch_service.mark_processed(
                    db=db,
                    task_id=task_id,
                    s3_qa_file_url=s3_urls.get("qa_file_url"),
                    s3_processed_file_url=s3_urls.get("processed_file_url")
                )
                self.batch_service.mark_uploaded_to_s3(db, task_id)
                
                # íŒŒì¸íŠœë‹ ì‹œì‘
                logger.info(f"ğŸ§  íŒŒì¸íŠœë‹ ì‹œì‘: task_id={task_id}")
                finetuning_task_id = await self.finetuning_service.start_finetuning_from_s3(
                    influencer_id=batch_job.influencer_id,
                    qa_task_id=task_id,
                    s3_qa_file_url=s3_urls["qa_file_url"]
                )
                
                if finetuning_task_id:
                    self.batch_service.mark_finetuning_started(db, task_id)
                    logger.info(f"ğŸ¯ íŒŒì¸íŠœë‹ ì‘ì—… ì‹œì‘ë¨: finetuning_task_id={finetuning_task_id}")
                else:
                    logger.error(f"íŒŒì¸íŠœë‹ ì‹œì‘ ì‹¤íŒ¨: task_id={task_id}")
                
                return True
            else:
                logger.error(f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨: task_id={task_id}")
                return False
                
        except Exception as e:
            logger.error(f"ê²°ê³¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (DB): task_id={task_id}, error={str(e)}")
            import traceback
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False

    def get_qa_task_status(self, task_id: str) -> Optional[QAGenerationTask]:
        """
        QA ìƒì„± ì‘ì—… ìƒíƒœ ì¡°íšŒ (í™˜ê²½ë³€ìˆ˜ì— ë”°ë¼ ë©”ëª¨ë¦¬ ë˜ëŠ” DBì—ì„œ ì¡°íšŒ)
        Args:
            task_id: ì‘ì—… ID
        Returns:
            ì‘ì—… ìƒíƒœ ì •ë³´
        """
        use_webhook = os.getenv('OPENAI_USE_WEBHOOK', 'true').lower() == 'true'
        
        if use_webhook:
            # ì›¹í›… ëª¨ë“œ: ë©”ëª¨ë¦¬ì—ì„œ ì¡°íšŒ
            logger.debug(f"ì›¹í›… ëª¨ë“œ - ë©”ëª¨ë¦¬ì—ì„œ ì‘ì—… ìƒíƒœ ì¡°íšŒ: task_id={task_id}")
            task = self.qa_generator.get_task_status(task_id)
            if task:
                logger.debug(f"ì›¹í›… ëª¨ë“œ - ì‘ì—… ì°¾ìŒ: task_id={task_id}, status={task.status.value}")
            else:
                logger.warning(f"ì›¹í›… ëª¨ë“œ - ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: task_id={task_id}")
            return task
        else:
            # í´ë§ ëª¨ë“œ: DBì—ì„œ ì¡°íšŒí•˜ì—¬ QAGenerationTask ê°ì²´ë¡œ ë³€í™˜
            logger.debug(f"í´ë§ ëª¨ë“œ - DBì—ì„œ ì‘ì—… ìƒíƒœ ì¡°íšŒ: task_id={task_id}")
            db = next(get_db())
            try:
                batch_job = self.batch_service.get_batch_job_by_task_id(db, task_id)
                if batch_job:
                    # DB ë°ì´í„°ë¥¼ QAGenerationTaskë¡œ ë³€í™˜
                    task = QAGenerationTask(
                        task_id=batch_job.task_id,
                        influencer_id=batch_job.influencer_id,
                        status=QAGenerationStatus(batch_job.status),
                        batch_id=batch_job.openai_batch_id,
                        total_qa_pairs=batch_job.total_qa_pairs,
                        generated_qa_pairs=batch_job.generated_qa_pairs,
                        error_message=batch_job.error_message,
                        s3_urls={
                            "qa_file_url": batch_job.s3_qa_file_url,
                            "processed_file_url": batch_job.s3_processed_file_url
                        } if batch_job.s3_qa_file_url else None,
                        created_at=batch_job.created_at,
                        updated_at=batch_job.updated_at
                    )
                    logger.debug(f"í´ë§ ëª¨ë“œ - DBì—ì„œ ì‘ì—… ì°¾ìŒ: task_id={task_id}, status={task.status.value}")
                    return task
                else:
                    logger.warning(f"í´ë§ ëª¨ë“œ - DBì—ì„œ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: task_id={task_id}")
                    return None
            finally:
                db.close()
    
    def get_all_qa_tasks(self) -> Dict[str, QAGenerationTask]:
        """
        ëª¨ë“  QA ìƒì„± ì‘ì—… ìƒíƒœ ì¡°íšŒ (í™˜ê²½ë³€ìˆ˜ì— ë”°ë¼ ë©”ëª¨ë¦¬ ë˜ëŠ” DBì—ì„œ ì¡°íšŒ)
        Returns:
            ëª¨ë“  ì‘ì—… ìƒíƒœ ì •ë³´
        """
        use_webhook = os.getenv('OPENAI_USE_WEBHOOK', 'true').lower() == 'true'
        
        if use_webhook:
            # ì›¹í›… ëª¨ë“œ: ë©”ëª¨ë¦¬ì—ì„œ ì¡°íšŒ
            return self.qa_generator.tasks
        else:
            # í´ë§ ëª¨ë“œ: DBì—ì„œ ì¡°íšŒí•˜ì—¬ QAGenerationTask ê°ì²´ë“¤ë¡œ ë³€í™˜
            db = next(get_db())
            try:
                batch_jobs = self.batch_service.get_all_batch_jobs(db)
                tasks = {}
                
                for batch_job in batch_jobs:
                    task = QAGenerationTask(
                        task_id=batch_job.task_id,
                        influencer_id=batch_job.influencer_id,
                        status=QAGenerationStatus(batch_job.status),
                        batch_id=batch_job.openai_batch_id,
                        total_qa_pairs=batch_job.total_qa_pairs,
                        generated_qa_pairs=batch_job.generated_qa_pairs,
                        error_message=batch_job.error_message,
                        s3_urls={
                            "qa_file_url": batch_job.s3_qa_file_url,
                            "processed_file_url": batch_job.s3_processed_file_url
                        } if batch_job.s3_qa_file_url else None,
                        created_at=batch_job.created_at,
                        updated_at=batch_job.updated_at
                    )
                    tasks[batch_job.task_id] = task
                
                return tasks
            finally:
                db.close()
    
    def is_task_running(self, task_id: str) -> bool:
        """
        ì‘ì—…ì´ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
        Args:
            task_id: ì‘ì—… ID
        Returns:
            ì‹¤í–‰ ì¤‘ ì—¬ë¶€
        """
        return task_id in self.running_tasks and not self.running_tasks[task_id].done()
    
    def cancel_task(self, task_id: str) -> bool:
        """
        ì‘ì—… ì·¨ì†Œ
        Args:
            task_id: ì‘ì—… ID
        Returns:
            ì·¨ì†Œ ì„±ê³µ ì—¬ë¶€
        """
        if task_id in self.running_tasks:
            task = self.running_tasks[task_id]
            if not task.done():
                task.cancel()
                logger.info(f"ì‘ì—… ì·¨ì†Œë¨: task_id={task_id}")
                return True
        return False
    
    def get_running_tasks_count(self) -> int:
        """
        ì‹¤í–‰ ì¤‘ì¸ ì‘ì—… ìˆ˜ ì¡°íšŒ
        Returns:
            ì‹¤í–‰ ì¤‘ì¸ ì‘ì—… ìˆ˜
        """
        return len([task for task in self.running_tasks.values() if not task.done()])
    
    def cleanup_completed_tasks(self):
        """
        ì™„ë£Œëœ ì‘ì—…ë“¤ì„ ì •ë¦¬
        """
        completed_task_ids = [
            task_id for task_id, task in self.running_tasks.items() 
            if task.done()
        ]
        
        for task_id in completed_task_ids:
            del self.running_tasks[task_id]
            logger.info(f"ì™„ë£Œëœ ì‘ì—… ì •ë¦¬: task_id={task_id}")


# ì „ì—­ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
background_task_manager = BackgroundTaskManager()


# ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… í•¨ìˆ˜ë“¤ (FastAPI Background Tasksì—ì„œ ì‚¬ìš©)
async def generate_influencer_qa_background(influencer_id: str):
    """
    ì¸í”Œë£¨ì–¸ì„œ QA ìƒì„± ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… í•¨ìˆ˜
    FastAPI Background Tasksì—ì„œ í˜¸ì¶œë¨
    Args:
        influencer_id: ì¸í”Œë£¨ì–¸ì„œ ID
    """
    try:
        await background_task_manager.start_qa_generation_task(influencer_id)
    except Exception as e:
        logger.error(f"ë°±ê·¸ë¼ìš´ë“œ QA ìƒì„± ì‘ì—… ì‹¤íŒ¨: influencer_id={influencer_id}, error={str(e)}", exc_info=True)


def get_background_task_manager() -> BackgroundTaskManager:
    """
    ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ë§¤ë‹ˆì € ì˜ì¡´ì„± ì£¼ì…ìš© í•¨ìˆ˜
    Returns:
        BackgroundTaskManager ì¸ìŠ¤í„´ìŠ¤
    """
    return background_task_manager