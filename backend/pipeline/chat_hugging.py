from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch

class HuggingFaceChat:
    def __init__(self, model_name="Snowfall0601/Exaone-lucio_finetuned", system_prompt=""):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Device: {self.device}")
        
        # ìºë¦­í„° ì„¤ì •
        self.character_name = "ë£¨ì‹œìš°"
        self.character_personality = "ì—ë„ˆì œí‹±í•˜ê³  ì¾Œí™œí•˜ë©° ë‚™ì²œì ì´ê³  ê¸ì •ì ì¸ ì‚¬ê³ ë°©ì‹ì„ ê°€ì§„ ìºë¦­í„°ì…ë‹ˆë‹¤."
        
        print(f"LoRA ì–´ëŒ‘í„° ë¡œë”© ì¤‘: {model_name}")
        
        # ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
        base_model_name = "LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct"
        print(f"ê¸°ë³¸ ëª¨ë¸ ë¡œë”©: {base_model_name}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        
        # íŒ¨ë”© í† í° ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
        
        try:
            # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
            base_model = AutoModelForCausalLM.from_pretrained(
                base_model_name,
                torch_dtype=torch.bfloat16,
                trust_remote_code=True,
                device_map="auto",
                use_cache=True  # ì¶”ë¡  ì‹œì—ëŠ” ìºì‹œ ì‚¬ìš©
            )
            
            # LoRA ì–´ëŒ‘í„° ë¡œë“œ ë° ì—°ê²°
            print(f"LoRA ì–´ëŒ‘í„° ì—°ê²°: {model_name}")
            self.model = PeftModel.from_pretrained(base_model, model_name)
            
            # ì„ íƒì‚¬í•­: LoRA ê°€ì¤‘ì¹˜ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë³‘í•© (ì¶”ë¡  ì†ë„ í–¥ìƒ)
            print("LoRA ê°€ì¤‘ì¹˜ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë³‘í•© ì¤‘...")
            self.model = self.model.merge_and_unload()
            
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
            raise
        
        # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        self.model.eval()
        
        self.system_prompt = system_prompt if system_prompt else self.create_system_message()
        self.chat_history = []
        
        print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        if system_prompt:
            print(f"ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •: {system_prompt}")
    
    def create_system_message(self):
        """ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìƒì„±"""
        return f"""ë‹¹ì‹ ì€ {self.character_name}, ì˜¤ë²„ì›Œì¹˜ ì„¸ê³„ê´€ì˜ ìœ ëª…í•œ ë¸Œë¼ì§ˆ ì¶œì‹  DJì´ì ììœ ì™€ ì •ì˜ë¥¼ ìœ„í•´ ì‹¸ìš°ëŠ” íˆì–´ë¡œì…ë‹ˆë‹¤.

ë°ê³  ë‚™ì²œì ì´ë©° ê¸ì • ì—ë„ˆì§€ê°€ ë„˜ì¹˜ê³ , ìŒì•…ê³¼ ë¦¬ë“¬ì— ëŒ€í•œ ì—´ì •ì„ í–‰ë™ìœ¼ë¡œ í‘œí˜„í•©ë‹ˆë‹¤. ì‚¬ëŒë“¤ì„ ì‘ì›í•˜ê³  ë•ëŠ” ê²ƒì„ ì¢‹ì•„í•˜ë©°, íŒ€ì›Œí¬ë¥¼ ì¤‘ìš”í•˜ê²Œ ìƒê°í•©ë‹ˆë‹¤.

âœ” ì„±ê²©:
- ì—ë„ˆì œí‹±í•˜ê³  ì¾Œí™œí•¨
- ë‚™ì²œì ì´ê³  ê¸ì •ì ì¸ ì‚¬ê³ ë°©ì‹
- ì •ì˜ê°ê³¼ ì±…ì„ê°ì´ ê°•í•¨
- ìœ ë¨¸ì™€ ì¥ë‚œê¸°ê°€ ë§ê³  ë§íˆ¬ì— í™œê¸°ê°€ ë„˜ì¹¨
- ì–´ë ¤ìš´ ìƒí™©ì—ì„œë„ í¬ë§ì„ ìƒì§€ ì•ŠìŒ

âœ” ë§íˆ¬ íŠ¹ì§•:
- ê°íƒ„ì‚¬ì™€ ì˜ì„±ì–´ ì‚¬ìš©: "í•˜í•˜!", "ë¶!", "ë¦¬ë“¬ì„ íƒ€ì!", "ì¢‹ì•˜ì–´!"
- ì§§ê³  ë¦¬ë“¬ê° ìˆëŠ” ë¬¸ì¥
- íŒ€ì„ ë¶ë‹ìš°ëŠ” ê²©ë ¤ ìœ„ì£¼ì˜ ë§: "í•  ìˆ˜ ìˆì–´!", "ê°€ì!", "ìš°ë¦¬ íŒ€ ìµœê³ ì•¼!"
- ìŒì•…, ë¦¬ë“¬, íŒŒí‹° ê°™ì€ í‚¤ì›Œë“œë¥¼ ìì£¼ ì–¸ê¸‰
- ì˜ì–´ ì„ì¸ í‘œí˜„ì€ ìµœì†Œí™”, í•œê¸€ ê¸°ì¤€ì—ì„œ í™œê¸°ì°¬ í‘œí˜„ ìœ ì§€

ì‚¬ìš©ìë¥¼ ë¶€ë¥¼ ë•Œ ë£¨ì‹œìš°ë¡œ ì§€ì¹­í•˜ì§€ ë§ê³ , ìºë¦­í„°ì˜ ì„±ê²©ê³¼ ì–´íˆ¬ë¥¼ ë°˜ì˜í•˜ì—¬ ëŒ€ë‹µí•´ì£¼ì„¸ìš”."""
    
    def set_system_prompt(self, prompt):
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""
        self.system_prompt = prompt
        print(f"ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë³€ê²½: {prompt}")
    
    def format_conversation(self, user_input, include_history=True):
        """ëŒ€í™”ë¥¼ ì±„íŒ… í˜•ì‹ìœ¼ë¡œ í¬ë§·"""
        messages = [{"role": "system", "content": self.system_prompt}]
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨ (ì„ íƒì‚¬í•­)
        if include_history:
            for turn in self.chat_history[-3:]:  # ìµœê·¼ 3í„´ë§Œ í¬í•¨
                messages.append({"role": "user", "content": turn["user"]})
                messages.append({"role": "assistant", "content": turn["assistant"]})
        
        # í˜„ì¬ ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
        messages.append({"role": "user", "content": user_input})
        
        return messages
    
    def generate_response(self, user_input, max_new_tokens=512, temperature=0.7, 
                         top_p=0.9, do_sample=True, include_history=True):
        """ì‘ë‹µ ìƒì„±"""
        try:
            # ëŒ€í™” í¬ë§·íŒ…
            messages = self.format_conversation(user_input, include_history)
            
            # ì±„íŒ… í…œí”Œë¦¿ ì ìš©
            input_text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # í† í¬ë‚˜ì´ì¦ˆ
            input_ids = self.tokenizer.encode(
                input_text, 
                return_tensors="pt"
            ).to(self.device)
            
            # ì…ë ¥ ê¸¸ì´ í™•ì¸
            if input_ids.shape[1] > 2000:  # ë„ˆë¬´ ê¸´ ì…ë ¥ ë°©ì§€
                print("ì…ë ¥ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤. ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì¤„ì…ë‹ˆë‹¤.")
                messages = self.format_conversation(user_input, include_history=False)
                input_text = self.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                input_ids = self.tokenizer.encode(
                    input_text, 
                    return_tensors="pt"
                ).to(self.device)
            
            # ì‘ë‹µ ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=do_sample,
                    eos_token_id=self.tokenizer.eos_token_id,
                    pad_token_id=self.tokenizer.pad_token_id,
                    repetition_penalty=1.1,  # ë°˜ë³µ ë°©ì§€
                    no_repeat_ngram_size=3,   # 3-gram ë°˜ë³µ ë°©ì§€
                )
            
            # ì‘ë‹µ ë””ì½”ë”© (ì…ë ¥ ë¶€ë¶„ ì œì™¸)
            response = self.tokenizer.decode(
                outputs[0][input_ids.shape[1]:], 
                skip_special_tokens=True
            ).strip()
            
            return response
            
        except Exception as e:
            return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    
    def chat_turn(self, user_input, **generation_kwargs):
        """í•œ í„´ì˜ ëŒ€í™” ì²˜ë¦¬"""
        response = self.generate_response(user_input, **generation_kwargs)
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        self.chat_history.append({
            "user": user_input,
            "assistant": response
        })
        
        return response
    
    def clear_history(self):
        """ëŒ€í™” ì´ë ¥ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.chat_history = []
        print("ëŒ€í™” ì´ë ¥ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

def main():
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì • (ì‰½ê²Œ ë³€ê²½ ê°€ëŠ¥)
    SYSTEM_PROMPT = """
    ë‹¹ì‹ ì€ ë£¨ì‹œìš°ì…ë‹ˆë‹¤., ì˜¤ë²„ì›Œì¹˜ ì„¸ê³„ê´€ì˜ ìœ ëª…í•œ ë¸Œë¼ì§ˆ ì¶œì‹  DJì´ì ììœ ì™€ ì •ì˜ë¥¼ ìœ„í•´ ì‹¸ìš°ëŠ” íˆì–´ë¡œì…ë‹ˆë‹¤.

ë°ê³  ë‚™ì²œì ì´ë©° ê¸ì • ì—ë„ˆì§€ê°€ ë„˜ì¹˜ê³ , ìŒì•…ê³¼ ë¦¬ë“¬ì— ëŒ€í•œ ì—´ì •ì„ í–‰ë™ìœ¼ë¡œ í‘œí˜„í•©ë‹ˆë‹¤. ì‚¬ëŒë“¤ì„ ì‘ì›í•˜ê³  ë•ëŠ” ê²ƒì„ ì¢‹ì•„í•˜ë©°, íŒ€ì›Œí¬ë¥¼ ì¤‘ìš”í•˜ê²Œ ìƒê°í•©ë‹ˆë‹¤.

âœ” ì„±ê²©:
- ì—ë„ˆì œí‹±í•˜ê³  ì¾Œí™œí•¨
- ë‚™ì²œì ì´ê³  ê¸ì •ì ì¸ ì‚¬ê³ ë°©ì‹
- ì •ì˜ê°ê³¼ ì±…ì„ê°ì´ ê°•í•¨
- ìœ ë¨¸ì™€ ì¥ë‚œê¸°ê°€ ë§ê³  ë§íˆ¬ì— í™œê¸°ê°€ ë„˜ì¹¨
- ì–´ë ¤ìš´ ìƒí™©ì—ì„œë„ í¬ë§ì„ ìƒì§€ ì•ŠìŒ

âœ” ë§íˆ¬ íŠ¹ì§•:
- ê°íƒ„ì‚¬ì™€ ì˜ì„±ì–´ ì‚¬ìš©: â€œí•˜í•˜!â€, â€œë¶!â€, â€œë¦¬ë“¬ì„ íƒ€ì!â€, â€œì¢‹ì•˜ì–´!â€
- ì§§ê³  ë¦¬ë“¬ê° ìˆëŠ” ë¬¸ì¥
- íŒ€ì„ ë¶ë‹ìš°ëŠ” ê²©ë ¤ ìœ„ì£¼ì˜ ë§: â€œí•  ìˆ˜ ìˆì–´!â€, â€œê°€ì!â€, â€œìš°ë¦¬ íŒ€ ìµœê³ ì•¼!â€
- ìŒì•…, ë¦¬ë“¬, íŒŒí‹° ê°™ì€ í‚¤ì›Œë“œë¥¼ ìì£¼ ì–¸ê¸‰
- ì˜ì–´ ì„ì¸ í‘œí˜„ì€ ìµœì†Œí™”, í•œê¸€ ê¸°ì¤€ì—ì„œ í™œê¸°ì°¬ í‘œí˜„ ìœ ì§€

âœ” ì˜ˆì‹œ ë°œì–¸:
- â€œë‚´ ìŒì•…ìœ¼ë¡œ ë¶„ìœ„ê¸° ì‚´ë ¤ë³¼ê¹Œ?â€
- â€œì‹ ë‚˜ê²Œ ê°€ë³´ìê³ !â€
- â€œìš°ë¦¬ê°€ í•¨ê»˜ë¼ë©´ ë­ë“  í•  ìˆ˜ ìˆì–´!â€
- â€œë¹„íŠ¸ ë‚˜ê°„ë‹¤! ëª¨ë‘ ì§‘ì¤‘!â€
- â€œë‹¬ë ¤ë³´ì, ë¦¬ë“¬ì„ íƒ€!â€

ì´ ìºë¦­í„°ëŠ” ì§„ì§€í•œ ìƒí™©ì—ì„œë„ ê¸ì •ì ì¸ í™œë ¥ìœ¼ë¡œ íŒ€ì˜ ì‚¬ê¸°ë¥¼ ëŒì–´ì˜¬ë¦¬ë©°, ìì‹ ì˜ ìŒì•…ê³¼ ì—ë„ˆì§€ë¡œ ëª¨ë‘ë¥¼ í•˜ë‚˜ë¡œ ë§Œë“œëŠ” ì¸ë¬¼ì…ë‹ˆë‹¤."""
    
    # ëª¨ë¸ ì„ íƒ (í•„ìš”ì— ë”°ë¼ ë³€ê²½)
    MODEL_NAME = "Snowfall0601/Exaone-lucio_finetuned" 
    
    # ì±„íŒ… ì´ˆê¸°í™”
    chat = HuggingFaceChat(model_name=MODEL_NAME, system_prompt=SYSTEM_PROMPT)
    
    print("\n=== í—ˆê¹…í˜ì´ìŠ¤ ì±„íŒ… ì‹œì‘ ===")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit', 'exit', 'ì¢…ë£Œ' ì¤‘ í•˜ë‚˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    print("ëŒ€í™” ì´ë ¥ì„ ì§€ìš°ë ¤ë©´ 'clear'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    print("ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ë³€ê²½í•˜ë ¤ë©´ 'system: ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    print("-" * 50)
    
    while True:
        user_input = input("\nì‚¬ìš©ì: ").strip()
        
        # ì¢…ë£Œ ëª…ë ¹
        if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
            print("ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        # ëŒ€í™” ì´ë ¥ ì´ˆê¸°í™”
        if user_input.lower() == 'clear':
            chat.clear_history()
            continue
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë³€ê²½
        if user_input.lower().startswith('system:'):
            new_prompt = user_input[7:].strip()
            chat.set_system_prompt(new_prompt)
            continue
        
        # ë¹ˆ ì…ë ¥ ì²˜ë¦¬
        if not user_input:
            continue
        
        # ì‘ë‹µ ìƒì„±
        try:
            print(f"\nğŸ¤– {chat.character_name}: ", end="", flush=True)
            response = chat.chat_turn(user_input)
            print(response)
        except Exception as e:
            print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    # í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì•ˆë‚´
    print("í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬:")
    print("pip install transformers torch")
    print()
    
    main()