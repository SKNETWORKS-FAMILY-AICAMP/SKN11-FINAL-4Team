# EXAONE 3.5 2.4B QLoRA 4ë¹„íŠ¸ ì–‘ìí™” íŒŒì¸íŠœë‹ (ê°œì„ ë¨)

import torch
import json
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling,
    EarlyStoppingCallback
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset
from huggingface_hub import HfApi
import os

# Windowsì—ì„œ bitsandbytes ë¬¸ì œ í•´ê²°
try:
    from transformers import BitsAndBytesConfig
    from peft import prepare_model_for_kbit_training
    BITSANDBYTES_AVAILABLE = True
    print("âœ… QLoRA (4ë¹„íŠ¸ ì–‘ìí™”) ì‚¬ìš© ê°€ëŠ¥")
except (ImportError, ModuleNotFoundError) as e:
    print(f"âš ï¸ BitsAndBytesConfig ë¶ˆê°€ëŠ¥: {e}")
    print("ğŸ’¡ QLoRA ëŒ€ì‹  ì¼ë°˜ LoRAë¡œ ì§„í–‰í•©ë‹ˆë‹¤")
    BITSANDBYTES_AVAILABLE = False
except Exception as e:
    print(f"âš ï¸ BitsAndBytesConfig ë¡œë”© ì˜¤ë¥˜: {e}")
    print("ğŸ’¡ QLoRA ëŒ€ì‹  ì¼ë°˜ LoRAë¡œ ì§„í–‰í•©ë‹ˆë‹¤")
    BITSANDBYTES_AVAILABLE = False

# GPU ì„¤ì • í™•ì¸
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Hugging Face í† í° ë° repo_id ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
HF_TOKEN = os.getenv("HF_TOKEN")
HF_REPO_ID = os.getenv("HF_REPO_ID", "Snowfall0601/Exaone-lucio_finetuned")  # ê¸°ë³¸ê°’ ì„¤ì •

if not HF_TOKEN:
    print("WARNING: HF_TOKEN í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
print(f"ì—…ë¡œë“œ ëŒ€ìƒ ì €ì¥ì†Œ: {HF_REPO_ID}")

class ExaoneDataPreprocessor:
    def __init__(self, tokenizer, max_length=2048):
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def create_chat_format(self, instruction, output, system_msg=None):
        """EXAONE ì±„íŒ… í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ë³€í™˜"""
        if system_msg is None:
            system_msg = "You are EXAONE model from LG AI Research, a helpful assistant."
        character_name = "ë£¨ì‹œìš°"    
        character_personality = "ìì‹ ê°ì´ ë„˜ì¹˜ëŠ” ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ë©°, ëŒ€ì‚¬ ëë§ˆë‹¤ 'ì', 'ì–´', 'ê³ ' ë“±ì˜ ì–´ë¯¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í™•ì‹ ì— ì°¬ í†¤ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
    
        messages = [
            {"role": "system", "content": """ë‹¹ì‹ ì€ ë£¨ì‹œìš°, ì˜¤ë²„ì›Œì¹˜ ì„¸ê³„ê´€ì˜ ìœ ëª…í•œ ë¸Œë¼ì§ˆ ì¶œì‹  DJì´ì ììœ ì™€ ì •ì˜ë¥¼ ìœ„í•´ ì‹¸ìš°ëŠ” íˆì–´ë¡œì…ë‹ˆë‹¤.

ë°ê³  ë‚™ì²œì ì´ë©° ê¸ì • ì—ë„ˆì§€ê°€ ë„˜ì¹˜ê³ , ìŒì•…ê³¼ ë¦¬ë“¬ì— ëŒ€í•œ ì—´ì •ì„ í–‰ë™ìœ¼ë¡œ í‘œí˜„í•©ë‹ˆë‹¤. ì‚¬ëŒë“¤ì„ ì‘ì›í•˜ê³  ë•ëŠ” ê²ƒì„ ì¢‹ì•„í•˜ë©°, íŒ€ì›Œí¬ë¥¼ ì¤‘ìš”í•˜ê²Œ ìƒê°í•©ë‹ˆë‹¤.

âœ” ì„±ê²©:
- ì—ë„ˆì œí‹±í•˜ê³  ì¾Œí™œí•¨
- ë‚™ì²œì ì´ê³  ê¸ì •ì ì¸ ì‚¬ê³ ë°©ì‹
- ì •ì˜ê°ê³¼ ì±…ì„ê°ì´ ê°•í•¨
- ìœ ë¨¸ì™€ ì¥ë‚œê¸°ê°€ ë§ê³  ë§íˆ¬ì— í™œê¸°ê°€ ë„˜ì¹¨
- ì–´ë ¤ìš´ ìƒí™©ì—ì„œë„ í¬ë§ì„ ìƒì§€ ì•ŠìŒ

âœ” ë§íˆ¬ íŠ¹ì§•:
- ê°íƒ„ì‚¬ì™€ ì˜ì„±ì–´ ì‚¬ìš©: â€œí•˜í•˜!â€, â€œë¶!â€, â€œë¦¬ë“¬ì„ íƒ€ì!â€, â€œì¢‹ì•˜ì–´!â€
- ì§§ê³  ë¦¬ë“¬ê° ìˆëŠ” ë¬¸ì¥
- íŒ€ì„ ë¶ë‹ìš°ëŠ” ê²©ë ¤ ìœ„ì£¼ì˜ ë§: â€œí•  ìˆ˜ ìˆì–´!â€, â€œê°€ì!â€, â€œìš°ë¦¬ íŒ€ ìµœê³ ì•¼!â€
- ìŒì•…, ë¦¬ë“¬, íŒŒí‹° ê°™ì€ í‚¤ì›Œë“œë¥¼ ìì£¼ ì–¸ê¸‰
- ì˜ì–´ ì„ì¸ í‘œí˜„ì€ ìµœì†Œí™”, í•œê¸€ ê¸°ì¤€ì—ì„œ í™œê¸°ì°¬ í‘œí˜„ ìœ ì§€

âœ” ì˜ˆì‹œ ë°œì–¸:
- â€œë‚´ ìŒì•…ìœ¼ë¡œ ë¶„ìœ„ê¸° ì‚´ë ¤ë³¼ê¹Œ?â€
- â€œì‹ ë‚˜ê²Œ ê°€ë³´ìê³ !â€
- â€œìš°ë¦¬ê°€ í•¨ê»˜ë¼ë©´ ë­ë“  í•  ìˆ˜ ìˆì–´!â€
- â€œë¹„íŠ¸ ë‚˜ê°„ë‹¤! ëª¨ë‘ ì§‘ì¤‘!â€
- â€œë‹¬ë ¤ë³´ì, ë¦¬ë“¬ì„ íƒ€!â€

ì´ ìºë¦­í„°ëŠ” ì§„ì§€í•œ ìƒí™©ì—ì„œë„ ê¸ì •ì ì¸ í™œë ¥ìœ¼ë¡œ íŒ€ì˜ ì‚¬ê¸°ë¥¼ ëŒì–´ì˜¬ë¦¬ë©°, ìì‹ ì˜ ìŒì•…ê³¼ ì—ë„ˆì§€ë¡œ ëª¨ë‘ë¥¼ í•˜ë‚˜ë¡œ ë§Œë“œëŠ” ì¸ë¬¼ì…ë‹ˆë‹¤.
"""},
            {"role": "user", "content": instruction},
            {"role": "assistant", "content": output}
        ]
        
        # ì±„íŒ… í…œí”Œë¦¿ ì ìš©
        formatted_text = self.tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=False
        )
        
        return formatted_text
    
    def tokenize_function(self, examples):
        """í† í°í™” í•¨ìˆ˜"""
        # ì…ë ¥ í…ìŠ¤íŠ¸ í† í°í™”
        tokenized = self.tokenizer(
            examples["text"],
            truncation=True,
            padding=False,
            max_length=self.max_length,
            return_tensors=None
        )
        
        # labelsì„ input_idsì™€ ë™ì¼í•˜ê²Œ ì„¤ì • (causal LM)
        tokenized["labels"] = tokenized["input_ids"].copy()
        
        return tokenized

def find_all_linear_names(model):
    """ëª¨ë¸ì—ì„œ ëª¨ë“  Linear ë ˆì´ì–´ ì´ë¦„ì„ ì°¾ëŠ” í•¨ìˆ˜"""
    cls = torch.nn.Linear
    lora_module_names = set()
    for name, module in model.named_modules():
        if isinstance(module, cls):
            names = name.split('.')
            lora_module_names.add(names[-1])
    
    # íŠ¹ì • ëª¨ë“ˆë“¤ì€ ì œì™¸ (ì¼ë°˜ì ìœ¼ë¡œ LoRAì— í¬í•¨í•˜ì§€ ì•ŠìŒ)
    if 'lm_head' in lora_module_names:
        lora_module_names.remove('lm_head')
    if 'embed_tokens' in lora_module_names:
        lora_module_names.remove('embed_tokens')
    
    return list(lora_module_names)

def create_qlora_config():
    """QLoRAë¥¼ ìœ„í•œ 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì • ìƒì„± (bitsandbytes ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°ì—ë§Œ)"""
    if not BITSANDBYTES_AVAILABLE:
        return None
    
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,                      # 4ë¹„íŠ¸ ì–‘ìí™” í™œì„±í™”
        bnb_4bit_use_double_quant=True,         # ì´ì¤‘ ì–‘ìí™”ë¡œ ë” ë†’ì€ ì •ë°€ë„
        bnb_4bit_quant_type="nf4",              # NormalFloat4 ì–‘ìí™” (QLoRA ê¶Œì¥)
        bnb_4bit_compute_dtype=torch.bfloat16,  # ê³„ì‚°ìš© ë°ì´í„° íƒ€ì…
    )
    return bnb_config

def load_model_and_tokenizer(model_name="LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct"):
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ (QLoRA ë˜ëŠ” ì¼ë°˜ LoRA)"""
    if BITSANDBYTES_AVAILABLE:
        print("ğŸš€ QLoRA 4ë¹„íŠ¸ ì–‘ìí™” ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
    else:
        print("ğŸš€ ì¼ë°˜ LoRA ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # íŒ¨ë”© í† í° ì„¤ì • (í•„ìš”í•œ ê²½ìš°)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    # ëª¨ë¸ ë¡œë“œ íŒŒë¼ë¯¸í„° ì„¤ì •
    model_kwargs = {
        "torch_dtype": torch.bfloat16,
        "trust_remote_code": True,
        "use_cache": False,  # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…ê³¼ í˜¸í™˜ì„±ì„ ìœ„í•´
    }
    
    # QLoRA ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°
    if BITSANDBYTES_AVAILABLE:
        try:
            bnb_config = create_qlora_config()
            model_kwargs.update({
                "quantization_config": bnb_config,  # 4ë¹„íŠ¸ ì–‘ìí™” ì ìš©
                "device_map": "auto",               # GPU ë©”ëª¨ë¦¬ì— ë§ê²Œ ìë™ ë°°ì¹˜
            })
            print("ğŸš€ QLoRA ì–‘ìí™” ì„¤ì • ì ìš©ë¨")
        except Exception as e:
            print(f"âš ï¸ QLoRA ì„¤ì • ì ìš© ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ ì¼ë°˜ LoRAë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤")
            BITSANDBYTES_AVAILABLE = False
            model_kwargs["device_map"] = "auto" if torch.cuda.is_available() else None
    else:
        # ì¼ë°˜ ëª¨ë“œì—ì„œëŠ” GPUë¡œ ì§ì ‘ ì´ë™
        model_kwargs["device_map"] = "auto" if torch.cuda.is_available() else None
    
    # ëª¨ë¸ ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)
    
    # QLoRA í›ˆë ¨ì„ ìœ„í•œ ëª¨ë¸ ì¤€ë¹„ (bitsandbytes ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°ì—ë§Œ)
    if BITSANDBYTES_AVAILABLE:
        try:
            model = prepare_model_for_kbit_training(
                model, 
                use_gradient_checkpointing=True  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…
            )
            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ - QLoRA 4ë¹„íŠ¸ ì–‘ìí™” ì ìš©")
            if hasattr(model, 'get_memory_footprint'):
                print(f"ğŸ“Š ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {model.get_memory_footprint() / 1024**3:.2f} GB")
        except Exception as e:
            print(f"âš ï¸ QLoRA ëª¨ë¸ ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ ì¼ë°˜ LoRAë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤")
            BITSANDBYTES_AVAILABLE = False
            # ì¼ë°˜ LoRA ëª¨ë“œë¡œ ëŒ€ì²´
            model.gradient_checkpointing_enable()
            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ - ì¼ë°˜ LoRA ëª¨ë“œ (ëŒ€ì²´)")
            if hasattr(model, 'get_memory_footprint'):
                print(f"ğŸ“Š ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {model.get_memory_footprint() / 1024**3:.2f} GB")
    else:
        # ì¼ë°˜ LoRA ëª¨ë“œì—ì„œë„ ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… í™œì„±í™”
        model.gradient_checkpointing_enable()
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ - ì¼ë°˜ LoRA ëª¨ë“œ")
        if hasattr(model, 'get_memory_footprint'):
            print(f"ğŸ“Š ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {model.get_memory_footprint() / 1024**3:.2f} GB")
    
    return model, tokenizer

def setup_qlora_config(model):
    """QLoRA ì „ìš© LoRA ì„¤ì • - 4ë¹„íŠ¸ ì–‘ìí™”ì— ìµœì í™”"""
    
    # ëª¨ë¸ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ Linear ëª¨ë“ˆë“¤ì„ ìë™ìœ¼ë¡œ ì°¾ê¸°
    target_modules = find_all_linear_names(model)
    print(f"ë°œê²¬ëœ Linear ëª¨ë“ˆë“¤: {target_modules}")
    
    # EXAONE ëª¨ë¸ì—ì„œ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ëª¨ë“ˆë“¤ë§Œ ì„ íƒ
    attention_modules = [name for name in target_modules if any(proj in name for proj in ['q_proj', 'k_proj', 'v_proj', 'o_proj'])]
    
    if not attention_modules:
        # attention ëª¨ë“ˆì´ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ì´ë¦„ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¼ë°˜ì ì¸ ì´ë¦„ë“¤ ì‹œë„
        common_names = ['query', 'key', 'value', 'dense', 'fc1', 'fc2', 'gate_proj', 'up_proj', 'down_proj']
        attention_modules = [name for name in target_modules if any(common in name for common in common_names)]
    
    if not attention_modules:
        # ê·¸ë˜ë„ ì—†ìœ¼ë©´ ì²˜ìŒ ëª‡ ê°œë§Œ ì‚¬ìš©
        attention_modules = target_modules[:4] if len(target_modules) >= 4 else target_modules
    
    print(f"QLoRAì— ì‚¬ìš©í•  ëª¨ë“ˆë“¤: {attention_modules}")
    
    # QLoRA ìµœì í™”ëœ ì„¤ì •
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=16,                           # QLoRAì—ì„œëŠ” ë” ë†’ì€ rank ì‚¬ìš© ê°€ëŠ¥ (4ë¹„íŠ¸ ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½)
        lora_alpha=32,                  # alpha = 2 * r (QLoRA ê¶Œì¥ ì„¤ì •)
        lora_dropout=0.1,              # ì•½ê°„ ë†’ì€ dropoutìœ¼ë¡œ overfitting ë°©ì§€
        target_modules=attention_modules,
        bias="none",                   # 4ë¹„íŠ¸ ì–‘ìí™”ì™€ í˜¸í™˜ì„±ì„ ìœ„í•´ bias ì‚¬ìš© ì•ˆí•¨
        use_rslora=True,               # RSLoRA ì‚¬ìš©ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
        init_lora_weights="gaussian",  # ê°€ìš°ì‹œì•ˆ ì´ˆê¸°í™”ë¡œ ì•ˆì •ì„± í–¥ìƒ
    )
    return lora_config

# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
def setup_lora_config(model):
    """í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­ - QLoRA ì„¤ì • ì‚¬ìš©"""
    return setup_qlora_config(model)

def prepare_dataset(tokenizer, max_length=1024):  # max_length ì¤„ì„
    """ë°ì´í„°ì…‹ ì¤€ë¹„ (ì˜ˆì‹œ ë°ì´í„°)"""
    
    # JSON íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ
    try:
        with open('new_qa.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
        data_list = data['data']
        print(f"JSON íŒŒì¼ì—ì„œ {len(data_list)}ê°œ ë°ì´í„° ë¡œë“œë¨")
    except FileNotFoundError:
        print("new_qa.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜ˆì‹œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        data_list = [
            {"question": "ì•ˆë…•í•˜ì„¸ìš”", "answer": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"},
            {"question": "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?", "answer": "ì£„ì†¡í•˜ì§€ë§Œ ì‹¤ì‹œê°„ ë‚ ì”¨ ì •ë³´ëŠ” ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."},
            {"question": "íŒŒì´ì¬ì´ ë­ì•¼?", "answer": "íŒŒì´ì¬ì€ ê°„ë‹¨í•˜ê³  ë°°ìš°ê¸° ì‰¬ìš´ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤."},
        ]
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    preprocessor = ExaoneDataPreprocessor(tokenizer, max_length)
    
    formatted_data = []
    for item in data_list:
        formatted_text = preprocessor.create_chat_format(
            item["question"], 
            item["answer"]
        )
        formatted_data.append({"text": formatted_text})
    
    # Dataset ê°ì²´ ìƒì„±
    dataset = Dataset.from_list(formatted_data)
    
    # í† í°í™”
    tokenized_dataset = dataset.map(
        preprocessor.tokenize_function,
        batched=True,
        remove_columns=dataset.column_names
    )
    
    # ë°ì´í„° ê²€ì¦ ë° ìˆ˜ì •
    def validate_and_fix_data(example):
        """ë°ì´í„° í˜•ì‹ ê²€ì¦ ë° ìˆ˜ì •"""
        # input_idsì™€ labelsê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
        if isinstance(example['input_ids'], list) and isinstance(example['labels'], list):
            # ì •ìƒì ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
            return example
        else:
            # ë¬¸ì œê°€ ìˆëŠ” ê²½ìš° ìˆ˜ì •
            if not isinstance(example['input_ids'], list):
                example['input_ids'] = example['input_ids'].tolist() if hasattr(example['input_ids'], 'tolist') else [example['input_ids']]
            if not isinstance(example['labels'], list):
                example['labels'] = example['labels'].tolist() if hasattr(example['labels'], 'tolist') else [example['labels']]
            return example
    
    tokenized_dataset = tokenized_dataset.map(validate_and_fix_data)
    
    return tokenized_dataset

def setup_training_arguments(output_dir="./exaone-qlora-results-system-custom"):
    """QLoRA ìµœì í™”ëœ í›ˆë ¨ ì¸ìˆ˜ ì„¤ì •"""
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=2,          # QLoRAë¡œ ë” í° ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê°€ëŠ¥
        gradient_accumulation_steps=8,          # ì´ ë°°ì¹˜ ì‚¬ì´ì¦ˆ = 2 * 8 = 16
        num_train_epochs=3,                     # QLoRAëŠ” ë” ì ì€ ì—í¬í¬ë¡œë„ íš¨ê³¼ì 
        learning_rate=5e-5,                     # QLoRA ê¶Œì¥ í•™ìŠµë¥  (ë” ë‚®ê²Œ)
        lr_scheduler_type="cosine",             # Cosine ìŠ¤ì¼€ì¤„ëŸ¬ë¡œ ë” ë¶€ë“œëŸ¬ìš´ í•™ìŠµ
        warmup_steps=50,                        # ë” ê¸´ warmup
        logging_steps=10,
        save_strategy="epoch",                  # epochë§ˆë‹¤ ì €ì¥
        eval_strategy="epoch",                  # ì¡°ê¸° ì¢…ë£Œë¥¼ ìœ„í•œ í‰ê°€ ì „ëµ
        load_best_model_at_end=True,           # ìµœì  ëª¨ë¸ ë¡œë“œ
        metric_for_best_model="loss",          # ìµœì  ëª¨ë¸ ê¸°ì¤€
        greater_is_better=False,               # lossëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
        bf16=True,                             # 4ë¹„íŠ¸ ì–‘ìí™”ì™€ í•¨ê»˜ bf16 ì‚¬ìš©
        gradient_checkpointing=True,           # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ í™œì„±í™”
        dataloader_pin_memory=False,
        remove_unused_columns=False,
        report_to="none",
        seed=42,
        optim="paged_adamw_8bit",              # QLoRA ìµœì í™”ëœ ì˜µí‹°ë§ˆì´ì €
        max_grad_norm=0.3,                     # QLoRA ê¶Œì¥ gradient clipping
        dataloader_num_workers=4,              # ë°ì´í„° ë¡œë”© ë³‘ë ¬í™”
        save_total_limit=1,
        ddp_find_unused_parameters=False,      # DDP ìµœì í™”
        group_by_length=True,                  # ê¸¸ì´ë³„ ê·¸ë£¹í™”ë¡œ íš¨ìœ¨ì„± í–¥ìƒ
        length_column_name="length",
        max_steps=-1,                          # epoch ê¸°ë°˜ í•™ìŠµ
        weight_decay=0.01,                     # ì •ê·œí™”
    )
    
    return training_args

def upload_to_huggingface(output_dir):
    """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ Hugging Face Hubì— ì—…ë¡œë“œ"""
    if not HF_TOKEN:
        print("HF_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•„ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    
    try:
        print(f"\n=== Hugging Face Hub ì—…ë¡œë“œ ì‹œì‘ ===")
        api = HfApi()
        
        # 1. ì €ì¥ì†Œ ìƒì„±
        print(f"ì €ì¥ì†Œ ìƒì„± ì¤‘: {HF_REPO_ID}")
        api.create_repo(
            repo_id=HF_REPO_ID,
            repo_type="model",
            private=False,
            token=HF_TOKEN,
            exist_ok=True,
        )
        
        # 2. ëª¨ë¸ íŒŒì¼ ì—…ë¡œë“œ
        print(f"ëª¨ë¸ ì—…ë¡œë“œ ì¤‘: {output_dir} -> {HF_REPO_ID}")
        api.upload_folder(
            repo_id=HF_REPO_ID,
            folder_path=output_dir,
            repo_type="model",
            token=HF_TOKEN,
        )
        
        print(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ! ëª¨ë¸ URL: https://huggingface.co/{HF_REPO_ID}")
        
    except Exception as e:
        print(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

def main():
    """QLoRA 4ë¹„íŠ¸ ì–‘ìí™” íŒŒì¸íŠœë‹ ë©”ì¸ í•¨ìˆ˜ (Windows í˜¸í™˜)"""
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    
    if BITSANDBYTES_AVAILABLE:
        print("=== QLoRA 4ë¹„íŠ¸ ì–‘ìí™” íŒŒì¸íŠœë‹ ì‹œì‘ ===")
    else:
        print("=== ì¼ë°˜ LoRA íŒŒì¸íŠœë‹ ì‹œì‘ (Windows í˜¸í™˜ ëª¨ë“œ) ===")
    
    try:
        # 1. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        model, tokenizer = load_model_and_tokenizer()
        
        # 2. ëª¨ë¸ êµ¬ì¡° í™•ì¸
        print("ëª¨ë¸ êµ¬ì¡° í™•ì¸ ì¤‘...")
        print(f"ëª¨ë¸ íƒ€ì…: {type(model)}")
        if BITSANDBYTES_AVAILABLE:
            print(f"4ë¹„íŠ¸ ì–‘ìí™” ì ìš©ë¨: {hasattr(model, 'quantization_config')}")
        else:
            print("ì¼ë°˜ LoRA ëª¨ë“œë¡œ ì§„í–‰")
        
        # 3. LoRA ì„¤ì • ë° ì ìš©
        lora_config = setup_qlora_config(model)  # QLoRA ì„¤ì •ì´ì§€ë§Œ bitsandbytes ì—†ìœ¼ë©´ ì¼ë°˜ LoRAë¡œ ë™ì‘
        model = get_peft_model(model, lora_config)
    
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
        print("ğŸ’¡ ì˜¤ë¥˜ í•´ê²° ë°©ë²•:")
        print("  1. requirements.txtì˜ íŒ¨í‚¤ì§€ë“¤ì´ ëª¨ë‘ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸")
        print("  2. Windowsì—ì„œëŠ” bitsandbytes ëŒ€ì‹  ì¼ë°˜ LoRAë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤")
        print("  3. GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•œì§€ í™•ì¸í•˜ì„¸ìš”")
        raise
    
    # 4. LoRA ì ìš© í›„ gradient checkpointing ë‹¤ì‹œ í™œì„±í™”
    if hasattr(model, 'enable_input_require_grads'):
        model.enable_input_require_grads()
    
    # 5. í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ì¶œë ¥
    model.print_trainable_parameters()
    
    # 6. gradient ì²´í¬ - ë” ìì„¸í•œ í™•ì¸
    print("\nGradient ì„¤ì • í™•ì¸:")
    trainable_params = 0
    all_params = 0
    
    for name, param in model.named_parameters():
        all_params += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
            print(f"  âœ“ {name}: {param.shape} (requires_grad=True)")
        else:
            print(f"  âœ— {name}: {param.shape} (requires_grad=False)")
    
    print(f"ì´ íŒŒë¼ë¯¸í„°: {all_params:,}")
    print(f"í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {trainable_params:,}")
    print(f"í›ˆë ¨ ê°€ëŠ¥ ë¹„ìœ¨: {100 * trainable_params / all_params:.4f}%")
    
    if trainable_params == 0:
        print("ERROR: í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # 7. ë°ì´í„°ì…‹ ì¤€ë¹„
    train_dataset = prepare_dataset(tokenizer)
    print(f"í›ˆë ¨ ë°ì´í„°ì…‹ í¬ê¸°: {len(train_dataset)}")
    
    # ë°ì´í„°ì…‹ì„ train/evalë¡œ ë¶„í•  (ì¡°ê¸° ì¢…ë£Œë¥¼ ìœ„í•œ validation ë°ì´í„° í•„ìš”)
    train_size = int(0.8 * len(train_dataset))
    eval_size = len(train_dataset) - train_size
    
    train_dataset_split = train_dataset.select(range(train_size))
    eval_dataset = train_dataset.select(range(train_size, train_size + eval_size))
    
    print(f"í›ˆë ¨ ë°ì´í„°: {len(train_dataset_split)}, ê²€ì¦ ë°ì´í„°: {len(eval_dataset)}")
    
    # 8. ë°ì´í„° ì½œë ˆì´í„° ì„¤ì • - ë” ì•ˆì „í•œ ë°©ì‹
    def data_collator(features):
        """ì»¤ìŠ¤í…€ ë°ì´í„° ì½œë ˆì´í„°"""
        # ì…ë ¥ ê¸¸ì´ í™•ì¸
        max_length = max(len(f["input_ids"]) for f in features)
        
        batch = {
            "input_ids": [],
            "attention_mask": [],
            "labels": []
        }
        
        for feature in features:
            input_ids = feature["input_ids"]
            labels = feature["labels"]
            
            # íŒ¨ë”© ì¶”ê°€
            padding_length = max_length - len(input_ids)
            
            # input_ids íŒ¨ë”©
            padded_input_ids = input_ids + [tokenizer.pad_token_id] * padding_length
            
            # attention_mask ìƒì„±
            attention_mask = [1] * len(input_ids) + [0] * padding_length
            
            # labels íŒ¨ë”© (-100ì€ loss ê³„ì‚°ì—ì„œ ë¬´ì‹œë¨)
            padded_labels = labels + [-100] * padding_length
            
            batch["input_ids"].append(padded_input_ids)
            batch["attention_mask"].append(attention_mask)
            batch["labels"].append(padded_labels)
        
        # í…ì„œë¡œ ë³€í™˜
        return {
            "input_ids": torch.tensor(batch["input_ids"], dtype=torch.long),
            "attention_mask": torch.tensor(batch["attention_mask"], dtype=torch.long),
            "labels": torch.tensor(batch["labels"], dtype=torch.long)
        }
    
    # 9. í›ˆë ¨ ì¸ìˆ˜ ì„¤ì •
    training_args = setup_training_arguments()
    
    # 10. ì¡°ê¸° ì¢…ë£Œ ì½œë°± ì„¤ì •
    early_stopping_callback = EarlyStoppingCallback(
        early_stopping_patience=2,  # 2 epoch ë™ì•ˆ ê°œì„ ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ
        early_stopping_threshold=0.01  # ìµœì†Œ ê°œì„  ì„ê³„ê°’
    )
    
    # 11. Trainer ì´ˆê¸°í™”
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset_split,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
        callbacks=[early_stopping_callback],
    )
    
    # 12. í›ˆë ¨ ì „ gradient í…ŒìŠ¤íŠ¸
    print("\n=== í›ˆë ¨ ì „ Gradient í…ŒìŠ¤íŠ¸ ===")
    model.train()
    sample_batch = next(iter(trainer.get_train_dataloader()))
    sample_batch = {k: v.to(model.device) for k, v in sample_batch.items()}
    
    # Forward pass
    outputs = model(**sample_batch)
    loss = outputs.loss
    print(f"Forward pass ì„±ê³µ, loss: {loss.item()}")
    
    # Backward pass í…ŒìŠ¤íŠ¸
    try:
        loss.backward()
        print("Backward pass ì„±ê³µ!")
        
        # Gradient í™•ì¸
        grad_found = False
        for name, param in model.named_parameters():
            if param.requires_grad and param.grad is not None:
                print(f"  Gradient found: {name}")
                grad_found = True
                break
        
        if not grad_found:
            print("WARNING: Gradientê°€ ê³„ì‚°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        
        # Gradient ì´ˆê¸°í™”
        model.zero_grad()
        
    except Exception as e:
        print(f"Backward pass ì‹¤íŒ¨: {e}")
        return
    
    # 13. QLoRA í›ˆë ¨ ì‹œì‘
    print("=== QLoRA 4ë¹„íŠ¸ ì–‘ìí™” í›ˆë ¨ ì‹œì‘ ===")
    try:
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
        if torch.cuda.is_available():
            print(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (í›ˆë ¨ ì „): {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        
        trainer.train()
        
        if torch.cuda.is_available():
            print(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (í›ˆë ¨ í›„): {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        
        print("ğŸ‰ QLoRA 4ë¹„íŠ¸ ì–‘ìí™” í›ˆë ¨ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ QLoRA í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        # ë” ìì„¸í•œ ë””ë²„ê¹… ì •ë³´
        print("\n=== QLoRA ë””ë²„ê¹… ì •ë³´ ===")
        print(f"ëª¨ë¸ íƒ€ì…: {type(model)}")
        print(f"Base model íƒ€ì…: {type(model.base_model) if hasattr(model, 'base_model') else 'N/A'}")
        print(f"4ë¹„íŠ¸ ì–‘ìí™”: {hasattr(model.base_model, 'quantization_config') if hasattr(model, 'base_model') else 'N/A'}")
        
        # PEFT ì„¤ì • í™•ì¸
        if hasattr(model, 'peft_config'):
            print(f"PEFT config: {model.peft_config}")
        
        # GPU ë©”ëª¨ë¦¬ ì •ë³´
        if torch.cuda.is_available():
            print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.memory_allocated() / 1024**3:.2f} GB / {torch.cuda.memory_reserved() / 1024**3:.2f} GB")
        
        raise
    
    # 14. ëª¨ë¸ ì €ì¥
    trainer.save_model()
    print(f"ëª¨ë¸ì´ {training_args.output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # 15. Hugging Face Hubì— ì—…ë¡œë“œ
    upload_to_huggingface(training_args.output_dir)

if __name__ == "__main__":
    # í›ˆë ¨ ì‹¤í–‰
    main()
    