# EXAONE 3.5 2.4B QLoRA 4ë¹„íŠ¸ ì–‘ìí™” íŒŒì¸íŠœë‹ (ê°œì„ ë¨)

import torch
import json
from typing import List, Dict
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling,
    EarlyStoppingCallback
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset
from huggingface_hub import HfApi
import os

# GPU ì„¤ì • í™•ì¸
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

class ExaoneDataPreprocessor:
    def __init__(self, tokenizer, max_length=2048):
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def create_chat_format(self, instruction, output, system_msg: str = None):
        """EXAONE ì±„íŒ… í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ë³€í™˜"""
        if system_msg is None:
            system_msg = "You are EXAONE model from LG AI Research, a helpful assistant."
        
        messages = [
            {"role": "system", "content": system_msg},
            {"role": "user", "content": instruction},
            {"role": "assistant", "content": output}
        ]
        
        # ì±„íŒ… í…œí”Œë¦¿ ì ìš©
        formatted_text = self.tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=False
        )
        
        return formatted_text
    
    def tokenize_function(self, examples):
        """í† í°í™” í•¨ìˆ˜"""
        # ì…ë ¥ í…ìŠ¤íŠ¸ í† í°í™”
        tokenized = self.tokenizer(
            examples["text"],
            truncation=True,
            padding=False,
            max_length=self.max_length,
            return_tensors=None
        )
        
        # labelsì„ input_idsì™€ ë™ì¼í•˜ê²Œ ì„¤ì • (causal LM)
        tokenized["labels"] = tokenized["input_ids"].copy()
        
        return tokenized

def find_all_linear_names(model):
    """ëª¨ë¸ì—ì„œ ëª¨ë“  Linear ë ˆì´ì–´ ì´ë¦„ì„ ì°¾ëŠ” í•¨ìˆ˜"""
    cls = torch.nn.Linear
    lora_module_names = set()
    for name, module in model.named_modules():
        if isinstance(module, cls):
            names = name.split('.')
            lora_module_names.add(names[-1])
    
    # íŠ¹ì • ëª¨ë“ˆë“¤ì€ ì œì™¸ (ì¼ë°˜ì ìœ¼ë¡œ LoRAì— í¬í•¨í•˜ì§€ ì•ŠìŒ)
    if 'lm_head' in lora_module_names:
        lora_module_names.remove('lm_head')
    if 'embed_tokens' in lora_module_names:
        lora_module_names.remove('embed_tokens')
    
    return list(lora_module_names)

def create_qlora_config():
    """QLoRAë¥¼ ìœ„í•œ 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì • ìƒì„± (bitsandbytes ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°ì—ë§Œ)"""

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,                      # 4ë¹„íŠ¸ ì–‘ìí™” í™œì„±í™”
        bnb_4bit_use_double_quant=True,         # ì´ì¤‘ ì–‘ìí™”ë¡œ ë” ë†’ì€ ì •ë°€ë„
        bnb_4bit_quant_type="nf4",              # NormalFloat4 ì–‘ìí™” (QLoRA ê¶Œì¥)
        bnb_4bit_compute_dtype=torch.bfloat16,  # ê³„ì‚°ìš© ë°ì´í„° íƒ€ì…
    )
    return bnb_config

def load_model_and_tokenizer(model_name="LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct"):
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ (QLoRA ë˜ëŠ” ì¼ë°˜ LoRA)"""
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # íŒ¨ë”© í† í° ì„¤ì • (í•„ìš”í•œ ê²½ìš°)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    # ëª¨ë¸ ë¡œë“œ íŒŒë¼ë¯¸í„° ì„¤ì •
    model_kwargs = {
        "torch_dtype": torch.bfloat16,
        "trust_remote_code": True,
        "use_cache": False,  # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…ê³¼ í˜¸í™˜ì„±ì„ ìœ„í•´
    }
    

    model_kwargs["device_map"] = "auto" if torch.cuda.is_available() else None
    
    # ëª¨ë¸ ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)
    
    # QLoRA í›ˆë ¨ì„ ìœ„í•œ ëª¨ë¸ ì¤€ë¹„ (bitsandbytes ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°ì—ë§Œ)
    model.gradient_checkpointing_enable()
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ - ì¼ë°˜ LoRA ëª¨ë“œ")
    if hasattr(model, 'get_memory_footprint'):
        print(f"ğŸ“Š ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {model.get_memory_footprint() / 1024**3:.2f} GB")
    
    return model, tokenizer

def setup_qlora_config(model):
    """QLoRA ì „ìš© LoRA ì„¤ì • - 4ë¹„íŠ¸ ì–‘ìí™”ì— ìµœì í™”"""
    
    # ëª¨ë¸ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ Linear ëª¨ë“ˆë“¤ì„ ìë™ìœ¼ë¡œ ì°¾ê¸°
    target_modules = find_all_linear_names(model)
    print(f"ë°œê²¬ëœ Linear ëª¨ë“ˆë“¤: {target_modules}")
    
    # EXAONE ëª¨ë¸ì—ì„œ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ëª¨ë“ˆë“¤ë§Œ ì„ íƒ
    attention_modules = [name for name in target_modules if any(proj in name for proj in ['q_proj', 'k_proj', 'v_proj', 'o_proj'])]
    
    if not attention_modules:
        # attention ëª¨ë“ˆì´ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ì´ë¦„ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¼ë°˜ì ì¸ ì´ë¦„ë“¤ ì‹œë„
        common_names = ['query', 'key', 'value', 'dense', 'fc1', 'fc2', 'gate_proj', 'up_proj', 'down_proj']
        attention_modules = [name for name in target_modules if any(common in name for common in common_names)]
    
    if not attention_modules:
        # ê·¸ë˜ë„ ì—†ìœ¼ë©´ ì²˜ìŒ ëª‡ ê°œë§Œ ì‚¬ìš©
        attention_modules = target_modules[:4] if len(target_modules) >= 4 else target_modules
    
    print(f"QLoRAì— ì‚¬ìš©í•  ëª¨ë“ˆë“¤: {attention_modules}")
    
    # QLoRA ìµœì í™”ëœ ì„¤ì •
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=16,                           # QLoRAì—ì„œëŠ” ë” ë†’ì€ rank ì‚¬ìš© ê°€ëŠ¥ (4ë¹„íŠ¸ ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½)
        lora_alpha=32,                  # alpha = 2 * r (QLoRA ê¶Œì¥ ì„¤ì •)
        lora_dropout=0.1,              # ì•½ê°„ ë†’ì€ dropoutìœ¼ë¡œ overfitting ë°©ì§€
        target_modules=attention_modules,
        bias="none",                   # 4ë¹„íŠ¸ ì–‘ìí™”ì™€ í˜¸í™˜ì„±ì„ ìœ„í•´ bias ì‚¬ìš© ì•ˆí•¨
        use_rslora=True,               # RSLoRA ì‚¬ìš©ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
        init_lora_weights="gaussian",  # ê°€ìš°ì‹œì•ˆ ì´ˆê¸°í™”ë¡œ ì•ˆì •ì„± í–¥ìƒ
    )
    return lora_config

# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
def setup_lora_config(model):
    """í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­ - QLoRA ì„¤ì • ì‚¬ìš©"""
    return setup_qlora_config(model)

def prepare_dataset(tokenizer, qa_data: List[Dict], system_message: str, max_length=1024):  # max_length ì¤„ì„
    """ë°ì´í„°ì…‹ ì¤€ë¹„ (ì˜ˆì‹œ ë°ì´í„°)"""
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    preprocessor = ExaoneDataPreprocessor(tokenizer, max_length)
    
    formatted_data = []
    for item in qa_data:
        formatted_text = preprocessor.create_chat_format(
            item.get('question', '').strip(), 
            item.get('answer', '').strip(),
            system_msg=system_message
        )
        formatted_data.append({"text": formatted_text})
    
    # Dataset ê°ì²´ ìƒì„±
    dataset = Dataset.from_list(formatted_data)
    
    # í† í°í™”
    tokenized_dataset = dataset.map(
        preprocessor.tokenize_function,
        batched=True,
        remove_columns=dataset.column_names
    )
    
    # ë°ì´í„° ê²€ì¦ ë° ìˆ˜ì •
    def validate_and_fix_data(example):
        """ë°ì´í„° í˜•ì‹ ê²€ì¦ ë° ìˆ˜ì •"""
        # input_idsì™€ labelsê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
        if isinstance(example["input_ids"], list) and isinstance(example["labels"], list):
            # ì •ìƒì ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
            return example
        else:
            # ë¬¸ì œê°€ ìˆëŠ” ê²½ìš° ìˆ˜ì •
            if not isinstance(example["input_ids"], list):
                example["input_ids"] = example["input_ids"].tolist() if hasattr(example["input_ids"], "tolist") else [example["input_ids"]]
            if not isinstance(example["labels"], list):
                example["labels"] = example["labels"].tolist() if hasattr(example["labels"], "tolist") else [example["labels"]]
            return example
    
    tokenized_dataset = tokenized_dataset.map(validate_and_fix_data)
    
    return tokenized_dataset

def setup_training_arguments(output_dir="./exaone-qlora-results-system-custom", training_epochs: int = 3):
    """QLoRA ìµœì í™”ëœ í›ˆë ¨ ì¸ìˆ˜ ì„¤ì •"""
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=2,          # QLoRAë¡œ ë” í° ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê°€ëŠ¥
        gradient_accumulation_steps=8,          # ì´ ë°°ì¹˜ ì‚¬ì´ì¦ˆ = 2 * 8 = 16
        num_train_epochs=training_epochs,       # QLoRAëŠ” ë” ì ì€ ì—í¬í¬ë¡œë„ íš¨ê³¼ì 
        learning_rate=5e-5,                     # QLoRA ê¶Œì¥ í•™ìŠµë¥  (ë” ë‚®ê²Œ)
        lr_scheduler_type="cosine",             # Cosine ìŠ¤ì¼€ì¤„ëŸ¬ë¡œ ë” ë¶€ë“œëŸ¬ìš´ í•™ìŠµ
        warmup_steps=50,                        # ë” ê¸´ warmup
        logging_steps=10,
        save_strategy="epoch",                  # epochë§ˆë‹¤ ì €ì¥
        eval_strategy="epoch",                  # ì¡°ê¸° ì¢…ë£Œë¥¼ ìœ„í•œ í‰ê°€ ì „ëµ
        load_best_model_at_end=True,           # ìµœì  ëª¨ë¸ ë¡œë“œ
        metric_for_best_model="loss",          # ìµœì  ëª¨ë¸ ê¸°ì¤€
        greater_is_better=False,               # lossëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
        bf16=True,                             # 4ë¹„íŠ¸ ì–‘ìí™”ì™€ í•¨ê»˜ bf16 ì‚¬ìš©
        gradient_checkpointing=True,           # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ í™œì„±í™”
        dataloader_pin_memory=False,
        remove_unused_columns=False,
        report_to="none",
        seed=42,
        optim="paged_adamw_8bit",              # QLoRA ìµœì í™”ëœ ì˜µí‹°ë§ˆì´ì €
        max_grad_norm=0.3,                     # QLoRA ê¶Œì¥ gradient clipping
        dataloader_num_workers=0,              # ë°ì´í„° ë¡œë”© ë³‘ë ¬í™”
        save_total_limit=1,
        ddp_find_unused_parameters=False,      # DDP ìµœì í™”
        group_by_length=True,                  # ê¸¸ì´ë³„ ê·¸ë£¹í™”ë¡œ íš¨ìœ¨ì„± í–¥ìƒ
        length_column_name="length",
        max_steps=-1,                          # epoch ê¸°ë°˜ í•™ìŠµ
        weight_decay=0.01,                     # ì •ê·œí™”
    )
    
    return training_args

def upload_to_huggingface(output_dir: str, hf_token: str, hf_repo_id: str):
    """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ Hugging Face Hubì— ì—…ë¡œë“œ"""
    if not hf_token:
        print("HF í† í°ì´ ì œê³µë˜ì§€ ì•Šì•„ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    
    try:
        print(f"\n=== Hugging Face Hub ì—…ë¡œë“œ ì‹œì‘ ===")
        api = HfApi()
        
        # 1. ì €ì¥ì†Œ ìƒì„±
        print(f"ì €ì¥ì†Œ ìƒì„± ì¤‘: {hf_repo_id}")
        api.create_repo(
            repo_id=hf_repo_id,
            repo_type="model",
            private=False,
            token=hf_token,
            exist_ok=True,
        )
        
        # 2. ëª¨ë¸ íŒŒì¼ ì—…ë¡œë“œ
        print(f"ëª¨ë¸ ì—…ë¡œë“œ ì¤‘: {output_dir} -> {hf_repo_id}")
        api.upload_folder(
            repo_id=hf_repo_id,
            folder_path=output_dir,
            repo_type="model",
            token=hf_token,
        )
        
        print(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ! ëª¨ë¸ URL: https://huggingface.co/{hf_repo_id}")
        return f"https://huggingface.co/{hf_repo_id}"
        
    except Exception as e:
        print(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

class DataCollator:
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def __call__(self, features):
        for f in features:
            f["input_ids"] = list(f["input_ids"])
            f["labels"] = list(f["labels"])
        max_length = max(len(f["input_ids"]) for f in features)
        batch = {
            "input_ids": [],
            "attention_mask": [],
            "labels": []
        }
        for feature in features:
            input_ids = feature["input_ids"]
            labels = feature["labels"]
            padding_length = max_length - len(input_ids)
            padded_input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length
            attention_mask = [1] * len(input_ids) + [0] * padding_length
            padded_labels = labels + [-100] * padding_length
            batch["input_ids"].append(padded_input_ids)
            batch["attention_mask"].append(attention_mask)
            batch["labels"].append(padded_labels)
        return {
            "input_ids": torch.tensor(batch["input_ids"], dtype=torch.long),
            "attention_mask": torch.tensor(batch["attention_mask"], dtype=torch.long),
            "labels": torch.tensor(batch["labels"], dtype=torch.long)
        }

def main(qa_data: List[Dict], system_message: str, hf_token: str, hf_repo_id: str, training_epochs: int = 3, output_dir: str = "./exaone-qlora-results-system-custom"):
    """QLoRA 4ë¹„íŠ¸ ì–‘ìí™” íŒŒì¸íŠœë‹ ë©”ì¸ í•¨ìˆ˜ (Windows í˜¸í™˜)"""
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    print(hf_token)
    try:
        # 1. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        model, tokenizer = load_model_and_tokenizer()
        
        # 2. ëª¨ë¸ êµ¬ì¡° í™•ì¸
        print("ëª¨ë¸ êµ¬ì¡° í™•ì¸ ì¤‘...")
        print(f"ëª¨ë¸ íƒ€ì…: {type(model)}")
        
        # 3. LoRA ì„¤ì • ë° ì ìš©
        lora_config = setup_qlora_config(model)  # QLoRA ì„¤ì •ì´ì§€ë§Œ bitsandbytes ì—†ìœ¼ë©´ ì¼ë°˜ LoRAë¡œ ë™ì‘
        model = get_peft_model(model, lora_config)
    
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
        print("ğŸ’¡ ì˜¤ë¥˜ í•´ê²° ë°©ë²•:")
        print("  1. requirements.txtì˜ íŒ¨í‚¤ì§€ë“¤ì´ ëª¨ë‘ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸")
        print("  2. Windowsì—ì„œëŠ” bitsandbytes ëŒ€ì‹  ì¼ë°˜ LoRAë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤")
        print("  3. GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•œì§€ í™•ì¸í•˜ì„¸ìš”")
        raise
    
    # 4. LoRA ì ìš© í›„ gradient checkpointing ë‹¤ì‹œ í™œì„±í™”
    if hasattr(model, 'enable_input_require_grads'):
        model.enable_input_require_grads()
    
    # 5. í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ì¶œë ¥
    model.print_trainable_parameters()
    
    # 6. gradient ì²´í¬ - ë” ìì„¸í•œ í™•ì¸
    print("\nGradient ì„¤ì • í™•ì¸:")
    trainable_params = 0
    all_params = 0
    
    for name, param in model.named_parameters():
        all_params += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
            print(f"  âœ“ {name}: {param.shape} (requires_grad=True)")
        else:
            print(f"  âœ— {name}: {param.shape} (requires_grad=False)")
    
    print(f"ì´ íŒŒë¼ë¯¸í„°: {all_params:,}")
    print(f"í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {trainable_params:,}")
    print(f"í›ˆë ¨ ê°€ëŠ¥ ë¹„ìœ¨: {100 * trainable_params / all_params:.4f}%")
    
    if trainable_params == 0:
        print("ERROR: í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return None
    
    # 7. ë°ì´í„°ì…‹ ì¤€ë¹„
    train_dataset = prepare_dataset(tokenizer, qa_data, system_message)
    print(f"í›ˆë ¨ ë°ì´í„°ì…‹ í¬ê¸°: {len(train_dataset)}")
    
    # ë°ì´í„°ì…‹ì„ train/evalë¡œ ë¶„í•  (ì¡°ê¸° ì¢…ë£Œë¥¼ ìœ„í•œ validation ë°ì´í„° í•„ìš”)
    train_size = int(0.8 * len(train_dataset))
    eval_size = len(train_dataset) - train_size
    
    train_dataset_split = train_dataset.select(range(train_size))
    eval_dataset = train_dataset.select(range(train_size, train_size + eval_size))
    
    print(f"í›ˆë ¨ ë°ì´í„°: {len(train_dataset_split)}, ê²€ì¦ ë°ì´í„°: {len(eval_dataset)}")
    
    # 8. ë°ì´í„° ì½œë ˆì´í„° ì„¤ì • - ë” ì•ˆì „í•œ ë°©ì‹ (í•¨ìˆ˜ëŠ” ì´ë¯¸ global scopeì— ìˆìŒ)
    
    # 9. í›ˆë ¨ ì¸ìˆ˜ ì„¤ì • (ê¸°ì¡´ ë¡œì§ëŒ€ë¡œ)
    training_args = setup_training_arguments(output_dir=output_dir, training_epochs=training_epochs)
    
    # 10. ì¡°ê¸° ì¢…ë£Œ ì½œë°± ì„¤ì •
    early_stopping_callback = EarlyStoppingCallback(
        early_stopping_patience=2,  # 2 epoch ë™ì•ˆ ê°œì„ ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ
        early_stopping_threshold=0.01  # ìµœì†Œ ê°œì„  ì„ê³„ê°’
    )
    
    # 11. Trainer ì´ˆê¸°í™”
    data_collator = DataCollator(tokenizer)
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset_split,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
        callbacks=[early_stopping_callback],
    )
    
    # 12. í›ˆë ¨ ì „ gradient í…ŒìŠ¤íŠ¸
    print("\n=== í›ˆë ¨ ì „ Gradient í…ŒìŠ¤íŠ¸ ===")
    model.train()
    sample_batch = next(iter(trainer.get_train_dataloader()))
    sample_batch = {k: v.to(model.device) for k, v in sample_batch.items()}
    
    # Forward pass
    outputs = model(**sample_batch)
    loss = outputs.loss
    print(f"Forward pass ì„±ê³µ, loss: {loss.item()}")
    
    # Backward pass í…ŒìŠ¤íŠ¸
    try:
        loss.backward()
        print("Backward pass ì„±ê³µ!")
        
        # Gradient í™•ì¸
        grad_found = False
        for name, param in model.named_parameters():
            if param.requires_grad and param.grad is not None:
                print(f"  Gradient found: {name}")
                grad_found = True
                break
        
        if not grad_found:
            print("WARNING: Gradientê°€ ê³„ì‚°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        
        # Gradient ì´ˆê¸°í™”
        model.zero_grad()
        
    except Exception as e:
        print(f"Backward pass ì‹¤íŒ¨: {e}")
        return None
    
    # 13. QLoRA í›ˆë ¨ ì‹œì‘
    print("=== QLoRA 4ë¹„íŠ¸ ì–‘ìí™” í›ˆë ¨ ì‹œì‘ ===")
    try:
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
        if torch.cuda.is_available():
            print(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (í›ˆë ¨ ì „): {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        
        trainer.train()
        
        if torch.cuda.is_available():
            print(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (í›ˆë ¨ í›„): {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        
        print("ğŸ‰ QLoRA 4ë¹„íŠ¸ ì–‘ìí™” í›ˆë ¨ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ QLoRA í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        # ë” ìì„¸í•œ ë””ë²„ê¹… ì •ë³´
        print("\n=== QLoRA ë””ë²„ê¹… ì •ë³´ ===")
        print(f"ëª¨ë¸ íƒ€ì…: {type(model)}")
        print(f"Base model íƒ€ì…: {type(model.base_model) if hasattr(model, 'base_model') else 'N/A'}")
        print(f"4ë¹„íŠ¸ ì–‘ìí™”: {hasattr(model.base_model, 'quantization_config') if hasattr(model, 'base_model') else 'N/A'}")
        
        # PEFT ì„¤ì • í™•ì¸
        if hasattr(model, 'peft_config'):
            print(f"PEFT config: {model.peft_config}")
        
        # GPU ë©”ëª¨ë¦¬ ì •ë³´
        if torch.cuda.is_available():
            print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.memory_allocated() / 1024**3:.2f} GB / {torch.cuda.memory_reserved() / 1024**3:.2f} GB")
        
        raise
    
    # 14. ëª¨ë¸ ì €ì¥
    trainer.save_model()
    print(f"ëª¨ë¸ì´ {training_args.output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # 15. Hugging Face Hubì— ì—…ë¡œë“œ
    model_url = upload_to_huggingface(training_args.output_dir, hf_token, hf_repo_id)
    return model_url