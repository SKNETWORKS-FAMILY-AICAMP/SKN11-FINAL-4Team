# EXAONE íŒŒì¸íŠœë‹ ëª¨ë¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤

import torch
import json
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import os

class ExaoneChatBot:
    def __init__(self, base_model_name="LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct", 
                 lora_path="./exaone-lora-results-3"):
        """
        EXAONE ì±„íŒ…ë´‡ ì´ˆê¸°í™”
        
        Args:
            base_model_name: ë² ì´ìŠ¤ ëª¨ë¸ ì´ë¦„
            lora_path: LoRA ì–´ëŒ‘í„°ê°€ ì €ì¥ëœ ê²½ë¡œ
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Device: {self.device}")
        
        # ìºë¦­í„° ì„¤ì •
        self.character_name = "ë£¨ì‹œìš°"
        self.character_personality = "ìì‹ ê°ì´ ë„˜ì¹˜ëŠ” ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ë©° ìê¸°ì• ê°€ ê°•í•œ ìºë¦­í„°ì…ë‹ˆë‹¤."
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬
        self.conversation_history = []
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.load_model(base_model_name, lora_path)
        
    def load_model(self, base_model_name, lora_path):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        print("ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        
        # íŒ¨ë”© í† í° ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
        
        try:
            # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
            base_model = AutoModelForCausalLM.from_pretrained(
                base_model_name,
                torch_dtype=torch.bfloat16,
                trust_remote_code=True,
                device_map="auto",
                use_cache=True  # ì¶”ë¡  ì‹œì—ëŠ” ìºì‹œ ì‚¬ìš©
            )
            
            # LoRA ì–´ëŒ‘í„° ë¡œë“œ ë° ë³‘í•©
            if os.path.exists(lora_path):
                print(f"LoRA ì–´ëŒ‘í„° ë¡œë”©: {lora_path}")
                self.model = PeftModel.from_pretrained(base_model, lora_path)
                
                # ì„ íƒì‚¬í•­: LoRA ê°€ì¤‘ì¹˜ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë³‘í•© (ì¶”ë¡  ì†ë„ í–¥ìƒ)
                print("LoRA ê°€ì¤‘ì¹˜ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë³‘í•© ì¤‘...")
                self.model = self.model.merge_and_unload()
            else:
                print(f"LoRA ì–´ëŒ‘í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {lora_path}")
                print("ë² ì´ìŠ¤ ëª¨ë¸ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                self.model = base_model
                
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
            raise
        
        # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        self.model.eval()
        print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    def create_system_message(self):
        """ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìƒì„±"""
        return f"ë‹¹ì‹ ì€ {self.character_name} ì…ë‹ˆë‹¤ ì‚¬ìš©ìë¥¼ ë¶€ë¥¼ë•Œ ë£¨ì‹œìš° ì§€ì¹­í•˜ì§€ ë§ˆì„¸ìš”. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ìºë¦­í„°ì˜ ì„±ê²©ê³¼ ì–´íˆ¬ë¥¼ ë°˜ì˜í•˜ì—¬ ëŒ€ë‹µí•´ì£¼ì„¸ìš”. ìºë¦­í„°ì˜ ì„±ê²©ê³¼ ì–´íˆ¬ëŠ” {self.character_personality}"
    
    def format_conversation(self, user_input, include_history=True):
        """ëŒ€í™”ë¥¼ ì±„íŒ… í˜•ì‹ìœ¼ë¡œ í¬ë§·"""
        messages = [{"role": "system", "content": self.create_system_message()}]
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨ (ì„ íƒì‚¬í•­)
        if include_history:
            for turn in self.conversation_history[-5:]:  # ìµœê·¼ 5í„´ë§Œ í¬í•¨
                messages.append({"role": "user", "content": turn["user"]})
                messages.append({"role": "assistant", "content": turn["assistant"]})
        
        # í˜„ì¬ ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
        messages.append({"role": "user", "content": user_input})
        
        return messages
    
    def generate_response(self, user_input, max_new_tokens=512, temperature=0.7, 
                         top_p=0.9, do_sample=True, include_history=True):
        """ì‘ë‹µ ìƒì„±"""
        try:
            # ëŒ€í™” í¬ë§·íŒ…
            messages = self.format_conversation(user_input, include_history)
            
            # ì±„íŒ… í…œí”Œë¦¿ ì ìš©
            input_text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # í† í¬ë‚˜ì´ì¦ˆ
            input_ids = self.tokenizer.encode(
                input_text, 
                return_tensors="pt"
            ).to(self.device)
            
            # ì…ë ¥ ê¸¸ì´ í™•ì¸
            if input_ids.shape[1] > 2000:  # ë„ˆë¬´ ê¸´ ì…ë ¥ ë°©ì§€
                print("ì…ë ¥ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤. ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì¤„ì…ë‹ˆë‹¤.")
                messages = self.format_conversation(user_input, include_history=False)
                input_text = self.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                input_ids = self.tokenizer.encode(
                    input_text, 
                    return_tensors="pt"
                ).to(self.device)
            
            # ì‘ë‹µ ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=do_sample,
                    eos_token_id=self.tokenizer.eos_token_id,
                    pad_token_id=self.tokenizer.pad_token_id,
                    repetition_penalty=1.1,  # ë°˜ë³µ ë°©ì§€
                    no_repeat_ngram_size=3,   # 3-gram ë°˜ë³µ ë°©ì§€
                )
            
            # ì‘ë‹µ ë””ì½”ë”© (ì…ë ¥ ë¶€ë¶„ ì œì™¸)
            response = self.tokenizer.decode(
                outputs[0][input_ids.shape[1]:], 
                skip_special_tokens=True
            ).strip()
            
            return response
            
        except Exception as e:
            return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    
    def chat_turn(self, user_input, **generation_kwargs):
        """í•œ í„´ì˜ ëŒ€í™” ì²˜ë¦¬"""
        response = self.generate_response(user_input, **generation_kwargs)
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        self.conversation_history.append({
            "user": user_input,
            "assistant": response
        })
        
        return response
    
    def clear_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.conversation_history = []
        print("ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def save_conversation(self, filename="conversation.json"):
        """ëŒ€í™” ë‚´ìš© ì €ì¥"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.conversation_history, f, ensure_ascii=False, indent=2)
            print(f"ëŒ€í™” ë‚´ìš©ì´ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def load_conversation(self, filename="conversation.json"):
        """ëŒ€í™” ë‚´ìš© ë¶ˆëŸ¬ì˜¤ê¸°"""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                self.conversation_history = json.load(f)
            print(f"{filename}ì—ì„œ ëŒ€í™” ë‚´ìš©ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
        except FileNotFoundError:
            print(f"{filename} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"ë¶ˆëŸ¬ì˜¤ê¸° ì¤‘ ì˜¤ë¥˜: {e}")

def interactive_chat():
    """ëŒ€í™”í˜• ì±„íŒ… ì¸í„°í˜ì´ìŠ¤"""
    print("=" * 60)
    print("ğŸ¤– EXAONE ë£¨ì‹œìš° ì±„íŒ…ë´‡")
    print("=" * 60)
    print("ëª…ë ¹ì–´:")
    print("  /clear    - ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”")
    print("  /save     - ëŒ€í™” ë‚´ìš© ì €ì¥")
    print("  /load     - ëŒ€í™” ë‚´ìš© ë¶ˆëŸ¬ì˜¤ê¸°")
    print("  /exit     - ì¢…ë£Œ")
    print("  /help     - ë„ì›€ë§")
    print("=" * 60)
    
    # ì±„íŒ…ë´‡ ì´ˆê¸°í™”
    try:
        chatbot = ExaoneChatBot()
    except Exception as e:
        print(f"ì±„íŒ…ë´‡ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    print(f"\nğŸ’¬ {chatbot.character_name}ì™€ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤!")
    print("ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
    
    while True:
        try:
            # ì‚¬ìš©ì ì…ë ¥
            user_input = input("\nğŸ‘¤ You: ").strip()
            
            if not user_input:
                continue
            
            # ëª…ë ¹ì–´ ì²˜ë¦¬
            if user_input.startswith('/'):
                command = user_input[1:].lower()
                
                if command == 'exit':
                    print("ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ì•ˆë…•íˆ ê°€ì„¸ìš”!")
                    break
                elif command == 'clear':
                    chatbot.clear_history()
                    continue
                elif command == 'save':
                    filename = input("ì €ì¥í•  íŒŒì¼ëª… (ê¸°ë³¸ê°’: conversation.json): ").strip()
                    if not filename:
                        filename = "conversation.json"
                    chatbot.save_conversation(filename)
                    continue
                elif command == 'load':
                    filename = input("ë¶ˆëŸ¬ì˜¬ íŒŒì¼ëª… (ê¸°ë³¸ê°’: conversation.json): ").strip()
                    if not filename:
                        filename = "conversation.json"
                    chatbot.load_conversation(filename)
                    continue
                elif command == 'help':
                    print("\nëª…ë ¹ì–´:")
                    print("  /clear    - ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”")
                    print("  /save     - ëŒ€í™” ë‚´ìš© ì €ì¥")
                    print("  /load     - ëŒ€í™” ë‚´ìš© ë¶ˆëŸ¬ì˜¤ê¸°")
                    print("  /exit     - ì¢…ë£Œ")
                    print("  /help     - ë„ì›€ë§")
                    continue
                else:
                    print("ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤. /helpë¥¼ ì…ë ¥í•˜ì—¬ ë„ì›€ë§ì„ í™•ì¸í•˜ì„¸ìš”.")
                    continue
            
            # ì‘ë‹µ ìƒì„±
            print(f"\nğŸ¤– {chatbot.character_name}: ", end="", flush=True)
            response = chatbot.chat_turn(user_input)
            print(response)
            
        except KeyboardInterrupt:
            print("\n\nëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ì•ˆë…•íˆ ê°€ì„¸ìš”!")
            break
        except Exception as e:
            print(f"\nì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            continue

def batch_chat(questions, lora_path="./exaone-lora-results-3"):
    """ì—¬ëŸ¬ ì§ˆë¬¸ì„ í•œ ë²ˆì— ì²˜ë¦¬í•˜ëŠ” ë°°ì¹˜ ëª¨ë“œ"""
    chatbot = ExaoneChatBot(lora_path=lora_path)
    
    results = []
    for i, question in enumerate(questions, 1):
        print(f"\nì§ˆë¬¸ {i}: {question}")
        response = chatbot.chat_turn(question)
        print(f"ë‹µë³€ {i}: {response}")
        
        results.append({
            "question": question,
            "response": response
        })
    
    return results

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ëŒ€í™”í˜• ëª¨ë“œ ì‹¤í–‰
    interactive_chat()
    
    # ë°°ì¹˜ ëª¨ë“œ ì˜ˆì‹œ (ì£¼ì„ ì²˜ë¦¬ë¨)
    """
    sample_questions = [
        "ì•ˆë…•í•˜ì„¸ìš”!",
        "íŒŒì´ì¬ìœ¼ë¡œ ê°„ë‹¨í•œ ê³„ì‚°ê¸°ë¥¼ ë§Œë“œëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
        "ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë– ì„¸ìš”?",
        "í”„ë¡œê·¸ë˜ë°ì„ ë°°ìš°ëŠ” ì¢‹ì€ ë°©ë²•ì´ ìˆì„ê¹Œìš”?"
    ]
    
    results = batch_chat(sample_questions)
    
    # ê²°ê³¼ ì €ì¥
    with open("batch_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    """
