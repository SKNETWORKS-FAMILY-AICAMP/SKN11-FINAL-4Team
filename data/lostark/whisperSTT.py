import openai
import os
import sys
import time
from pathlib import Path
import json
from datetime import datetime
from pydub import AudioSegment
import tempfile
import math

def get_audio_duration(audio_path):
    """ì˜¤ë””ì˜¤ íŒŒì¼ì˜ ê¸¸ì´ë¥¼ êµ¬í•˜ëŠ” í•¨ìˆ˜"""
    try:
        audio = AudioSegment.from_file(audio_path)
        return len(audio) / 1000  # ì´ˆ ë‹¨ìœ„
    except Exception as e:
        print(f"ì˜¤ë””ì˜¤ ê¸¸ì´ í™•ì¸ ì‹¤íŒ¨: {e}")
        return 0

def split_audio_file(audio_path, max_size_mb=23):
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ì„ 25MB ì´í•˜ ì²­í¬ë¡œ ë¶„í• 
    
    Args:
        audio_path (str): ì›ë³¸ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        max_size_mb (int): ìµœëŒ€ íŒŒì¼ í¬ê¸° (MB)
    
    Returns:
        list: ë¶„í• ëœ íŒŒì¼ ê²½ë¡œë“¤
    """
    
    file_size_mb = os.path.getsize(audio_path) / (1024 * 1024)
    
    # íŒŒì¼ì´ ì¶©ë¶„íˆ ì‘ìœ¼ë©´ ë¶„í• í•˜ì§€ ì•ŠìŒ
    if file_size_mb <= max_size_mb:
        return [audio_path]
    
    print(f"íŒŒì¼ í¬ê¸°ê°€ {file_size_mb:.1f}MBì´ë¯€ë¡œ ë¶„í• í•©ë‹ˆë‹¤...")
    
    # ì˜¤ë””ì˜¤ ë¡œë“œ
    audio = AudioSegment.from_file(audio_path)
    duration_ms = len(audio)
    duration_minutes = duration_ms / (1000 * 60)
    
    print(f"ì˜¤ë””ì˜¤ ê¸¸ì´: {duration_minutes:.1f}ë¶„")
    
    # ì²­í¬ í¬ê¸° ê³„ì‚° (10ë¶„ ë‹¨ìœ„ë¡œ ë¶„í• )
    chunk_duration_ms = 10 * 60 * 1000  # 10ë¶„
    overlap_ms = 2000  # 2ì´ˆ ê²¹ì¹¨
    
    chunks = []
    temp_dir = tempfile.mkdtemp()
    
    start_ms = 0
    chunk_num = 0
    
    while start_ms < duration_ms:
        chunk_num += 1
        end_ms = min(start_ms + chunk_duration_ms, duration_ms)
        
        # ì²­í¬ ì¶”ì¶œ
        chunk = audio[start_ms:end_ms]
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        chunk_path = os.path.join(temp_dir, f"chunk_{chunk_num:03d}.mp3")
        chunk.export(chunk_path, format="mp3", bitrate="128k")
        
        chunks.append(chunk_path)
        
        chunk_size = os.path.getsize(chunk_path) / (1024 * 1024)
        chunk_duration = len(chunk) / (1000 * 60)
        
        print(f"ì²­í¬ {chunk_num}: {chunk_duration:.1f}ë¶„, {chunk_size:.1f}MB")
        
        # ë‹¤ìŒ ì²­í¬ ì‹œì‘ì  (ê²¹ì¹¨ ê³ ë ¤)
        start_ms = end_ms - overlap_ms
        
        if end_ms >= duration_ms:
            break
    
    print(f"ì´ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")
    return chunks

def transcribe_audio_with_api(audio_path, api_key, model="whisper-1", language=None, response_format="verbose_json", temperature=0):
    """
    OpenAI APIë¥¼ ì‚¬ìš©í•´ì„œ ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    
    Args:
        audio_path (str): ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        api_key (str): OpenAI API í‚¤
        model (str): ì‚¬ìš©í•  ëª¨ë¸
        language (str): ì–¸ì–´ ì½”ë“œ
        response_format (str): ì‘ë‹µ í˜•ì‹
        temperature (float): ì˜¨ë„ ì„¤ì •
    
    Returns:
        dict: ë³€í™˜ ê²°ê³¼
    """
    
    if not os.path.exists(audio_path):
        raise FileNotFoundError(f"ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}")
    
    # íŒŒì¼ í¬ê¸° í™•ì¸
    file_size = os.path.getsize(audio_path)
    file_size_mb = file_size / (1024 * 1024)
    
    if file_size > 25 * 1024 * 1024:
        raise ValueError(f"íŒŒì¼ í¬ê¸°ê°€ 25MBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤: {file_size_mb:.2f}MB")
    
    # OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
    client = openai.OpenAI(api_key=api_key)
    
    print(f"ì²˜ë¦¬ ì¤‘: {os.path.basename(audio_path)} ({file_size_mb:.2f}MB)")
    
    start_time = time.time()
    
    try:
        with open(audio_path, "rb") as audio_file:
            # API í˜¸ì¶œ ë§¤ê°œë³€ìˆ˜ ì„¤ì •
            transcription_params = {
                "file": audio_file,
                "model": model,
                "response_format": response_format,
                "temperature": temperature
            }
            
            # ì–¸ì–´ê°€ ì§€ì •ëœ ê²½ìš° ì¶”ê°€
            if language:
                transcription_params["language"] = language
            
            # API í˜¸ì¶œ
            transcript = client.audio.transcriptions.create(**transcription_params)
            
        end_time = time.time()
        processing_time = end_time - start_time
        print(f"API ì²˜ë¦¬ ì™„ë£Œ: {processing_time:.2f}ì´ˆ")
        
        # ê²°ê³¼ ì •ë³´ ì¶œë ¥
        if response_format == "verbose_json":
            if hasattr(transcript, 'text'):
                text_length = len(transcript.text)
                segments_count = len(transcript.segments) if hasattr(transcript, 'segments') else 0
                print(f"í…ìŠ¤íŠ¸ ê¸¸ì´: {text_length}ì, ì„¸ê·¸ë¨¼íŠ¸: {segments_count}ê°œ")
        
        return transcript
        
    except Exception as e:
        print(f"API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
        raise

def transcribe_long_audio(audio_path, api_key, model="whisper-1", language=None, temperature=0):
    """
    ê¸´ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë¶„í• í•´ì„œ ì „ì²´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    
    Args:
        audio_path (str): ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        api_key (str): OpenAI API í‚¤
        model (str): ëª¨ë¸ëª…
        language (str): ì–¸ì–´ ì½”ë“œ
        temperature (float): ì˜¨ë„
    
    Returns:
        dict: ì „ì²´ ë³€í™˜ ê²°ê³¼
    """
    
    # ì˜¤ë””ì˜¤ ê¸¸ì´ í™•ì¸
    duration = get_audio_duration(audio_path)
    print(f"ì „ì²´ ì˜¤ë””ì˜¤ ê¸¸ì´: {duration/60:.1f}ë¶„")
    
    # ì˜ˆìƒ ë¹„ìš© ê³„ì‚°
    estimated_cost = (duration / 60) * 0.006
    print(f"ì˜ˆìƒ ë¹„ìš©: ${estimated_cost:.4f}")
    
    # íŒŒì¼ ë¶„í• 
    chunk_paths = split_audio_file(audio_path)
    
    # ê° ì²­í¬ ì²˜ë¦¬
    all_segments = []
    full_text = ""
    total_duration = 0
    
    try:
        for i, chunk_path in enumerate(chunk_paths, 1):
            print(f"\n=== ì²­í¬ {i}/{len(chunk_paths)} ì²˜ë¦¬ ì¤‘ ===")
            
            # ì²­í¬ ì²˜ë¦¬
            result = transcribe_audio_with_api(
                chunk_path, 
                api_key, 
                model=model, 
                language=language, 
                response_format="verbose_json", 
                temperature=temperature
            )
            
            # ê²°ê³¼ ì²˜ë¦¬
            if hasattr(result, 'text'):
                chunk_text = result.text.strip()
                full_text += chunk_text + " "
                
                # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ ìˆ˜ì§‘ (ì‹œê°„ ì˜¤í”„ì…‹ ì¡°ì •)
                if hasattr(result, 'segments'):
                    for segment in result.segments:
                        adjusted_segment = {
                            'start': segment.start + total_duration,
                            'end': segment.end + total_duration,
                            'text': segment.text
                        }
                        all_segments.append(adjusted_segment)
                
                # ì²­í¬ ê¸¸ì´ ëˆ„ì 
                chunk_duration = get_audio_duration(chunk_path)
                total_duration += chunk_duration
                
                print(f"ì²­í¬ {i} í…ìŠ¤íŠ¸ ê¸¸ì´: {len(chunk_text)}ì")
            
    finally:
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        for chunk_path in chunk_paths:
            if chunk_path != audio_path and os.path.exists(chunk_path):
                try:
                    os.remove(chunk_path)
                except:
                    pass
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
        if len(chunk_paths) > 1:
            temp_dir = os.path.dirname(chunk_paths[0])
            try:
                os.rmdir(temp_dir)
            except:
                pass
    
    # ê²°ê³¼ ë°˜í™˜
    result = {
        'text': full_text.strip(),
        'segments': all_segments,
        'language': language,
        'duration': total_duration
    }
    
    print(f"\nğŸ‰ ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ìµœì¢… í…ìŠ¤íŠ¸ ê¸¸ì´: {len(result['text'])}ì")
    print(f"ì´ ì„¸ê·¸ë¨¼íŠ¸: {len(all_segments)}ê°œ")
    print(f"ì´ ì²˜ë¦¬ ì‹œê°„: {total_duration/60:.1f}ë¶„")
    
    return result

def save_transcript_result(result, output_path, include_timestamps=True):
    """ë³€í™˜ ê²°ê³¼ë¥¼ ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
    
    # í…ìŠ¤íŠ¸ íŒŒì¼
    with open(f"{output_path}.txt", "w", encoding="utf-8") as f:
        f.write(result['text'])
    print(f"í…ìŠ¤íŠ¸ ì €ì¥: {output_path}.txt")
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ í…ìŠ¤íŠ¸
    if include_timestamps and 'segments' in result:
        with open(f"{output_path}_timestamps.txt", "w", encoding="utf-8") as f:
            for segment in result['segments']:
                start_time = f"{int(segment['start']//60):02d}:{int(segment['start']%60):02d}"
                f.write(f"[{start_time}] {segment['text'].strip()}\n")
        print(f"íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥: {output_path}_timestamps.txt")
    
    # JSON íŒŒì¼
    with open(f"{output_path}.json", "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f"JSON ì €ì¥: {output_path}.json")

def get_api_key():
    """API í‚¤ ê°€ì ¸ì˜¤ê¸°"""
    api_key = os.getenv('OPENAI_API_KEY')
    
    if not api_key:
        api_key = input("OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        
        if not api_key:
            raise ValueError("API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    return api_key

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    if len(sys.argv) < 2:
        print("=== OpenAI Whisper API STT ë³€í™˜ê¸° (ê¸´ ìŒì„± ì§€ì›) ===")
        print("\nì‚¬ìš©ë²•:")
        print("   python whisper_long_stt.py <ì˜¤ë””ì˜¤_íŒŒì¼> [ì–¸ì–´]")
        print("\nì˜ˆì‹œ:")
        print("   python whisper_long_stt.py audio.mp3")
        print("   python whisper_long_stt.py audio.mp3 ko")
        print("\nì–¸ì–´ ì½”ë“œ: ko(í•œêµ­ì–´), en(ì˜ì–´), ja(ì¼ë³¸ì–´), zh(ì¤‘êµ­ì–´) ë“±")
        print("\ní™˜ê²½ë³€ìˆ˜ ì„¤ì •:")
        print("   export OPENAI_API_KEY=your_api_key_here")
        print("\níŠ¹ì§•:")
        print("   - ê¸´ ìŒì„± íŒŒì¼ ìë™ ë¶„í•  ì²˜ë¦¬")
        print("   - íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ ê²°ê³¼")
        print("   - ì „ì²´ ìŒì„± ì™„ì „ ë³€í™˜")
        return
    
    try:
        # API í‚¤ ê°€ì ¸ì˜¤ê¸°
        api_key = get_api_key()
        
        audio_file = sys.argv[1]
        language = sys.argv[2] if len(sys.argv) > 2 else None
        
        # ìŒì„± ì¸ì‹ ì‹¤í–‰
        result = transcribe_long_audio(
            audio_file, 
            api_key,
            language=language
        )
        
        # ì¶œë ¥ íŒŒì¼ ì´ë¦„ ìƒì„±
        output_name = os.path.splitext(audio_file)[0]
        
        # ê²°ê³¼ ì €ì¥
        save_transcript_result(result, output_name, include_timestamps=True)
        
        # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
        print(f"\nğŸ“ ë³€í™˜ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")
        preview = result['text'][:300] + "..." if len(result['text']) > 300 else result['text']
        print(f"{preview}")
        
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    main()